{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adda584-f8d0-4475-8031-ef560ec0eae1",
   "metadata": {},
   "source": [
    "# FBSDE\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "a810fbc7-9c2f-41fd-83b9-cc3987654ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate\n",
    "from keras import Model, initializers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "55ed80e7-951e-467c-9d2f-ae92e6453acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ead24874-5164-405d-bf64-f13d7c6e8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_paths = 256\n",
    "n_timesteps = 10\n",
    "n_dimensions = 4\n",
    "n_factors = 2\n",
    "T = 1.\n",
    "dt = T / n_timesteps\n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "315af737-b3d1-4196-a36d-d82d67ef2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convention: x = (s, alpha, q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "139ef913-e310-4583-b090-4826fd30d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1.\n",
    "lp = 1.\n",
    "lm = 1.\n",
    "k = 0.01\n",
    "sigma = 1.\n",
    "zeta = 1.\n",
    "phi = 1.\n",
    "psi = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed6c3c18-cbb2-400a-9f4b-02bac81a1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z):\n",
    "    return [\n",
    "        x[1],\n",
    "        -eta * x[0],\n",
    "        lp * tf.exp(-1 + k * y[2] / y[3] + x[0] * k) - lm * tf.exp(-1 - k * y[2] / y[3] - x[0] * k),\n",
    "        lp * (1./k - y[2] / y[3]) * tf.exp(-1 + k * y[2] / y[3] + x[0] * k) - lm * (-1./k - y[2] / y[3]) * tf.exp(-1. - k * y[2] / y[3] - x[0] * k),\n",
    "    ]\n",
    "\n",
    "def s(t, x, y, z):\n",
    "    return [[sigma, 0], [0, zeta], [0, 0], [0, 0]]\n",
    "\n",
    "def dH_dx(t, x, y, z):\n",
    "    return [\n",
    "        y[3] * lp * tf.exp(-1. + k * y[2] / y[3] + x[0] * k) - y[3] * lm * tf.exp(-1. - k * y[2] / y[3] - x[0] * k),\n",
    "        y[0] - eta * y[1],\n",
    "        -2. * phi * x[2],\n",
    "        0.\n",
    "    ]\n",
    "\n",
    "def dg_dx(x):\n",
    "    return [x[2], 0., x[0], x[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "c2ebbb08-c357-4d31-bd93-35074b4f7fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_294), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 4) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_294), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 4) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_295), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 4) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_295), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 4) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "\n",
    "inputs_dW = Input(shape=(n_timesteps, n_factors))\n",
    "\n",
    "x0 = tf.constant([[0., 0., 0., 0.]])\n",
    "y0 = tf.Variable([[1., 1., 1., 1.]])\n",
    "\n",
    "x = x0\n",
    "y = y0\n",
    "\n",
    "z = concatenate([x, y])\n",
    "z = Dense(10, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-2))(z)\n",
    "z = Dense(n_dimensions * n_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-2))(z)\n",
    "z = Reshape((n_dimensions, n_factors))(z)\n",
    "\n",
    "paths += [[x, y, z]]\n",
    "\n",
    "for i in range(n_timesteps):\n",
    "            \n",
    "    def dX(x, y, z, dw):\n",
    "        \n",
    "        def drift(arg):\n",
    "            x, y, z = arg\n",
    "            return tf.math.multiply(b(i*dt, x, y, z), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, dw = arg\n",
    "            return tf.tensordot(s(i*dt, x, y, z), dw[i], [[1], [0]])\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, dw))\n",
    "        \n",
    "        return a0 + a1\n",
    "\n",
    "    def dY(x, y, z, dw):\n",
    "        \n",
    "        def drift(arg):\n",
    "            x, y, z = arg\n",
    "            return tf.math.multiply(dH_dx(i*dt, x, y, z), -dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, dw = arg\n",
    "            return tf.tensordot(z, dw[i], [[1], [0]])\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, dw))\n",
    "        \n",
    "        return a0 + a1\n",
    "    \n",
    "    x, y = (\n",
    "        Lambda(lambda r: r[0] + dX(r[0], r[1], r[2], r[3]))([x, y, z, inputs_dW]),\n",
    "        Lambda(lambda r: r[1] + dY(r[0], r[1], r[2], r[3]))([x, y, z, inputs_dW]),\n",
    "    )\n",
    "    \n",
    "    # we don't train z for the last time step; keep for consistency\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(10, activation='relu')(z)\n",
    "    z = Dense(n_dimensions * n_factors, activation='relu')(z)\n",
    "    z = Reshape((n_dimensions, n_factors))(z)\n",
    "\n",
    "    paths += [[x, y, z]]\n",
    "    \n",
    "outputs_loss = Lambda(lambda r: r[1] - tf.transpose(tf.vectorized_map(dg_dx, r[0])))([x, y])\n",
    "model_loss = Model(inputs_dW, outputs_loss)\n",
    "model_loss.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "# skips the first time step\n",
    "outputs_paths = tf.stack([tf.stack([p[0] for p in paths[1:]], axis=1), tf.stack([p[1] for p in paths[1:]], axis=1)] + [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_factors)], axis=2)\n",
    "model_paths = Model(inputs_dW, outputs_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf827ce-a9b3-4f56-bf18-6bf78e9f9fe2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a974bef6-b86d-4fd0-8ce5-2639da0e7f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_factors))\n",
    "target = tf.zeros((n_paths, 4))\n",
    "model_loss.fit(dW, target, batch_size=batch_size, epochs=epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011bd1d-4eb1-4671-b023-f3739ae1ce35",
   "metadata": {},
   "source": [
    "# Display paths and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84eae2e4-a26c-415b-b894-3c9b38534341",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss(dW).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5bf926-5b40-4d81-9431-aae2ee7ea622",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = model_paths(dW).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1127c7ce-db2a-482f-87f4-c2e0601cc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%9.4g\" % x))\n",
    "tf.transpose(paths[3, :, :, :], (1, 2, 0)).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyDeepLearning-1.1",
   "language": "python",
   "name": "pydeeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
