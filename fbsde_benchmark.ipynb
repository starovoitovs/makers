{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc06f6d1",
   "metadata": {},
   "source": [
    "# FBSDE\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2a52e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate, Layer\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "ea6fecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "dd71b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_paths = 4096\n",
    "n_timesteps = 25\n",
    "n_dimensions = 100\n",
    "n_factors = 100\n",
    "T = 1.\n",
    "dt = T / n_timesteps\n",
    "batch_size = n_paths\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1815e-1461-4872-8c35-408cf1ad7d89",
   "metadata": {},
   "source": [
    "# Initial value layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "ffbd26b8-560e-44b2-bad6-9ecab7d27ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialValue(Layer):\n",
    "    \n",
    "    def __init__(self, y0):\n",
    "        super().__init__()\n",
    "        self.y0 = tf.Variable(y0, trainable=True)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        print(type(inputs))\n",
    "        return self.y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "2996b179-bf97-45fb-b893-95a9117116fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_52), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=() dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "313/313 [==============================] - 0s 532us/step - loss: 99.7464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5e610542b0>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs = Input(shape=(1,))\n",
    "y0 = InitialValue(10.)(inputs)\n",
    "outputs = Lambda(lambda x: x[0] + x[1])([y0, inputs])\n",
    "model = Model([inputs], outputs)\n",
    "model.compile(loss='mse', optimizer='adam')\n",
    "model.fit(tf.zeros((10000,)), tf.random.normal((10000,)), epochs=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a9bb2-fa76-4340-87d5-3f8edff51e58",
   "metadata": {},
   "source": [
    "# Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "d4ebf547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z):\n",
    "    return tf.fill((n_dimensions,), 0.)\n",
    "\n",
    "def s(t, x, y, z):\n",
    "    # discard non-diagonal elements of z\n",
    "    return n_dimensions * tf.exp(-1/n_dimensions * tf.reduce_sum(x)) * tf.linalg.diag(tf.linalg.diag_part(z))\n",
    "\n",
    "def f(t, x, y, z):\n",
    "    # take only diagonal elements of z\n",
    "    return tf.repeat(tf.exp(-1./n_dimensions * tf.reduce_sum(x)) * tf.reduce_sum(tf.square(tf.linalg.diag_part(z))), n_dimensions)\n",
    "\n",
    "def g(x):\n",
    "    return tf.repeat(tf.exp(1./n_dimensions * tf.reduce_sum(x)), n_dimensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "35de6b6c-540e-470c-957e-f435b197d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dX(t, x, y, z, dw):\n",
    "    \n",
    "    def drift(arg):\n",
    "        x, y, z = arg\n",
    "        return tf.math.multiply(b(t, x, y, z), dt)\n",
    "    a0 = tf.vectorized_map(drift, (x, y, z))\n",
    "        \n",
    "    def noise(arg):\n",
    "        x, y, z, dw = arg\n",
    "        return tf.tensordot(s(t, x, y, z), dw[i], [[1], [0]])\n",
    "    a1 = tf.vectorized_map(noise, (x, y, z, dw))\n",
    "            \n",
    "    return a0 + a1\n",
    "\n",
    "def dY(t, x, y, z, dw):\n",
    "\n",
    "    def drift(arg):\n",
    "        x, y, z = arg\n",
    "        return tf.math.multiply(f(t, x, y, z), dt)\n",
    "    a0 = tf.vectorized_map(drift, (x, y, z))\n",
    "\n",
    "    def noise(arg):\n",
    "        x, y, z, dw = arg\n",
    "        return tf.tensordot(z, dw[i], [[1], [0]])\n",
    "    a1 = tf.vectorized_map(noise, (x, y, z, dw))\n",
    "    \n",
    "    return a0 + a1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712e6b3-b6ac-4607-b7a1-1d804d531f69",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "ce1f9427-7dbe-479c-8008-a9f239bac9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "WARNING:tensorflow:AutoGraph could not transform <function hx at 0x7f5e998cac10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function hx at 0x7f5e998cac10> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function hy at 0x7f5e998caca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function hy at 0x7f5e998caca0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "\n",
    "inputs_dW = Input(shape=(n_timesteps, n_factors))\n",
    "\n",
    "x0 = tf.constant([[1.] * n_dimensions])\n",
    "y0 = [[2.] * n_dimensions]\n",
    "\n",
    "x = x0\n",
    "y = InitialValue(y0)(inputs_dW)\n",
    "\n",
    "z = concatenate([x, y])\n",
    "z = Dense(10, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-2), name='z1_0')(z)\n",
    "z = Dense(n_dimensions * n_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-2), name='z2_0')(z)\n",
    "z = Reshape((n_dimensions, n_factors), name='reshape_0')(z)\n",
    "\n",
    "paths += [[x, y, z]]\n",
    "\n",
    "@tf.function\n",
    "def hx(r):\n",
    "    return r[0] + dX(i * dt, r[0], r[1], r[2], r[3])\n",
    "\n",
    "@tf.function\n",
    "def hy(r):\n",
    "    return r[1] + dY(i * dt, r[0], r[1], r[2], r[3])\n",
    "\n",
    "for i in range(n_timesteps):\n",
    "    \n",
    "    x, y = (\n",
    "        Lambda(hx, name=f'dx_{i}')([x, y, z, inputs_dW]),\n",
    "        Lambda(hy, name=f'dy_{i}')([x, y, z, inputs_dW]),\n",
    "    )\n",
    "    \n",
    "    # we don't train z for the last time step; keep for consistency\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(10, activation='relu', name=f'z1_{i+1}')(z)\n",
    "    z = Dense(n_dimensions * n_factors, activation='relu', name=f'z2_{i+1}')(z)\n",
    "    z = Reshape((n_dimensions, n_factors), name=f'reshape_{i+1}')(z)\n",
    "\n",
    "    paths += [[x, y, z]]\n",
    "\n",
    "outputs_loss = Lambda(lambda r: r[1] - tf.vectorized_map(g, r[0]))([x, y])\n",
    "outputs_paths = tf.stack([tf.stack([p[0] for p in paths[1:]], axis=1), tf.stack([p[1] for p in paths[1:]], axis=1)] + [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_factors)], axis=2)\n",
    "\n",
    "model_loss = Model(inputs_dW, outputs_loss)\n",
    "model_loss.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "# skips the first time step\n",
    "model_paths = Model(inputs_dW, outputs_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5cf0e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "56a90e15-3142-4b85-9946-b92135d1d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_factors))\n",
    "target = tf.zeros((n_paths, n_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "a03915f5-7578-42d4-a664-e90869d936c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (dx_0), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 100) dtype=float32, numpy=array([[        2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (dy_0), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 100) dtype=float32, numpy=array([[        2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2,         2]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f5e5a6ac4c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10000,), dtype=float32, numpy=array([      nan,      5.66,     5.942,    -23.54,     42.16,  -0.07529,     8.756,     -32.3,     30.13,    -13.68,    -29.93,     4.265,     18.97,     -39.3,    0.3957,    -11.22,    -3.341,     -11.4,     -4.85,     16.21,     37.96,    -3.696,      20.3,     27.29,     -8.65,    -9.411,     22.47,      41.2,      25.1,     10.14, ...,     20.97,    -32.85,    -4.268,    -23.43,    -12.89,    -14.14,      19.3,    -3.287,     26.91,    -18.63,     17.82,     42.09,     19.93,    -26.41,     7.683,     1.232,   -0.8981,   -0.2112,    -6.968,     5.219,     23.84,     29.57,     17.95,     6.645,     3.439,     18.98,     17.14,    -27.03,    -22.66,       nan], dtype=float32)>"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for exploding gradients before training\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = model_loss(dW)\n",
    "    \n",
    "variables = model_loss.variables[-1]\n",
    "tape.gradient(loss, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e5ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "callback = ModelCheckpoint('_models/weights{epoch:03d}.h5', period=1, save_weights_only=True, overwrite=True)\n",
    "model_loss.fit(dW, target, batch_size=batch_size, epochs=1000, callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844d499",
   "metadata": {},
   "source": [
    "# Display paths and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f13900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bad model\n",
    "model_loss.load_weights('_models/weights001.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "71d789ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (dx_0), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 4) dtype=float32, numpy=array([[        2,         2,         2,         2]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (dy_0), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 4) dtype=float32, numpy=array([[        2,         2,         2,         2]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f5e7f18bc10> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  -0.6775,    0.4698,     1.808,     0.314],\n",
       "       [   -1.284,    0.1527,   -0.5152,   -0.6432],\n",
       "       [   0.6276,  -0.02144,  -0.05538,     0.198],\n",
       "       [ -0.01756,   -0.7259,   -0.6613,    0.1513],\n",
       "       [   -0.837,    -1.986,     -1.49,    -1.563],\n",
       "       [   -3.305,    -3.932,    -2.581,    -3.619],\n",
       "       [   0.8182,    -1.574,     -1.22,  -0.03084],\n",
       "       [    1.065,    0.6896,    0.3729,    0.7629],\n",
       "       [   -0.405,   -0.8818,    -0.792,   -0.2468],\n",
       "       [  0.05031,    -2.038,    -1.549,   -0.3511],\n",
       "       [   -7.949,    -13.99,    -14.17,    -18.09],\n",
       "       [   0.2198,   -0.1332,    -2.649,   -0.2122],\n",
       "       [   0.2255,    0.6244,   -0.3724,    0.5045],\n",
       "       [ -0.08177,     1.052,    -2.819,    -1.141],\n",
       "       [    1.363,    0.2058,  -0.08906,    0.3074],\n",
       "       [  -0.2603,   -0.4994,   -0.8175,   -0.8452],\n",
       "       [  -0.1544,     0.193,   -0.1946,    0.1098],\n",
       "       [    1.345,     1.538,    -1.182,  -0.07162],\n",
       "       [   0.0834,   -0.8688,   -0.3108,   -0.2078],\n",
       "       [   -2.012,     1.509,    0.1491,   -0.9334],\n",
       "       [  -0.3212,    0.2521,   -0.7818,   -0.6925],\n",
       "       [  -0.8306,   -0.3687,     1.166,   -0.2167],\n",
       "       [  -0.9682,   0.01326,    0.6828,   -0.2308],\n",
       "       [  -0.5696,   -0.3114,   -0.5708,   -0.6488],\n",
       "       [  0.03312, -0.004787,   -0.3739,    0.1056],\n",
       "       [   0.7204,      1.68,    0.2682,     1.029],\n",
       "       [   -2.176,   -0.9418,     1.063,    0.2949],\n",
       "       [  -0.9004,     1.555,    0.7358,    0.9153],\n",
       "       [  -0.2477,   -0.9054,    0.7781,   -0.5987],\n",
       "       [  -0.7015,      1.22,     0.556,    0.5363],\n",
       "       ...,\n",
       "       [  -0.1249,     0.547,   -0.4122,   0.09305],\n",
       "       [  -0.4355,     0.673,     1.452,  -0.02201],\n",
       "       [    -1.85,    -8.786,    -4.334,    -6.795],\n",
       "       [  -0.2688,  -0.09743,    -0.948,  -0.09406],\n",
       "       [ -0.08825,    -2.906,    -1.395,    -1.061],\n",
       "       [   -13.46,    -1.617,    -19.63,    -10.55],\n",
       "       [   -1.249,     -0.65,     1.433,   -0.6603],\n",
       "       [   -2.322,     -1.36,    -4.051,    -2.355],\n",
       "       [    1.404,     1.789,  -0.02607,    0.7454],\n",
       "       [  -0.1129,     -1.83,   -0.8602,   -0.5782],\n",
       "       [  -0.9443,    0.2748,    0.6345,   -0.5849],\n",
       "       [  -0.4533,   -0.6128,   -0.2326,   -0.4646],\n",
       "       [   -1.376,   -0.5907,   -0.2895,   -0.7846],\n",
       "       [  -0.9078,    -2.286,    -3.238,    -3.172],\n",
       "       [    1.335,    -1.792,    -1.467,     1.457],\n",
       "       [  -0.2687,     0.343,   -0.1488,    0.2094],\n",
       "       [    0.179,    -0.807,   -0.3846,  -0.06703],\n",
       "       [   -1.021,  -0.06119,     1.485,  -0.05933],\n",
       "       [  -0.6216,   -0.9696,   -0.1742,   -0.1646],\n",
       "       [  -0.3975,  -0.03196,  0.003091,    0.1745],\n",
       "       [  -0.3523,    0.8611,     1.518,    0.5638],\n",
       "       [ -0.02961,      -1.4,    -0.955,    0.2039],\n",
       "       [   0.8098,     1.164,    0.8895,    0.9131],\n",
       "       [    0.898,    0.6729,    -5.312,    -3.361],\n",
       "       [  -0.1492,    -1.552,   -0.7633,    -0.341],\n",
       "       [   0.5886,    -0.258,    -0.318,    0.7586],\n",
       "       [  0.06088,    0.2981,   -0.1975,  -0.01843],\n",
       "       [   -1.067,   -0.5797,   -0.0591,    -0.688],\n",
       "       [  -0.3635,     1.632,    -1.343,     1.061],\n",
       "       [  -0.1356,    -1.141,   -0.6866,   -0.2382]], dtype=float32)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model_loss(dW).numpy()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "99074637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.framework.ops.EagerTensor'>\n"
     ]
    }
   ],
   "source": [
    "paths = model_paths(dW).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "e5b0f8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[        1,         1,         1,         1,    0.1636,    -1.572,    -2.478,    -2.478, -3.005e+04,       nan],\n",
       "        [        1,         1,         1,         1,         1,     1.028,     1.028,     1.028,     1.028,       nan],\n",
       "        [        1,    0.7938,    0.7938,    0.2201,   -0.4654,   -0.8909,    -1.686,    -10.62,     -2075,       nan],\n",
       "        [        1,         1,         1,     0.801,     0.801,     0.801,    -4.139,    -16.56,    -16.56,       nan]],\n",
       "\n",
       "       [[        2,     1.605,     1.605,     1.171,    0.8077,  -0.04179,    0.1739,     0.385,      2758,       inf],\n",
       "        [        2,     1.819,     1.819,     1.354,    0.5568,    0.3871,   -0.3507,  -0.02773,      2761,       inf],\n",
       "        [        2,    0.9443,    0.9443,    0.5957,  0.005049,   -0.9188,    -1.236,    -1.056,      2762,       inf],\n",
       "        [        2,     1.971,     1.971,    0.2938,    0.2504,   -0.3511,    -1.047,    -1.385,      2762,       inf]],\n",
       "\n",
       "       [[        0,         0,         0,    0.3476,    0.4931,     0.151,         0,      4.58,         0,         0],\n",
       "        [   0.1431,         0,         0,    0.6453,         0,    0.2918,         0,     1.039,         0,         0],\n",
       "        [   0.7395,         0,         0,    0.1988,    0.5397,    0.2923,         0,         0,      6526,         0],\n",
       "        [        0,         0,     1.209,    0.1152,    0.4885,   0.08887,    0.5807,         0,         0,         0]],\n",
       "\n",
       "       [[        0,         0,     1.029,    0.6675,    0.2871,    0.4495,         0,     1.051,      7534,         0],\n",
       "        [        0,         0,         0,         0,    0.1297,         0,         0,         0,         0,         0],\n",
       "        [   0.3558,         0,         0,         0,         0,     1.246,   0.03176,         0,         0,         0],\n",
       "        [0.0008673,         0,         0,     1.547,         0,         0,    0.7992,         0,         0,         0]],\n",
       "\n",
       "       [[   0.7515,         0,         0,         0,    0.5069,         0,     1.213,   0.01852,      8720,         0],\n",
       "        [        0,         0,    0.2126,         0,    0.3901,    0.6025,    0.5341,         0,         0,         0],\n",
       "        [   0.2681,         0,    0.7084,    0.6977,    0.2961,    0.3247,    0.6929,    0.7709,         0,         0],\n",
       "        [  0.06043,         0,   0.04482,   0.09106,         0,         0,         0,         0, 1.225e+04,         0]],\n",
       "\n",
       "       [[ 0.009643,         0,     1.006,         0,         0,    0.1851,         0,         0,         0,         0],\n",
       "        [        0,         0,     0.704,         0,         0,    0.9768,    0.4549,     1.882,      4219,         0],\n",
       "        [        0,         0,         0,         0,    0.1913,     0.645,    0.5727,         0,         0,         0],\n",
       "        [        0,         0,    0.2407,         0,         0,     1.975,    0.9438,         0,     342.8,         0]]], dtype=float32)"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%9.4g\" % x))\n",
    "tf.transpose(paths[262, :, :, :], (1, 2, 0)).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
