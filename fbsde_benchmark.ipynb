{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5a38d80-2cf3-4fc4-aa1a-5586984ab75d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# FBSDE\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327.\n",
    "\n",
    "For standard $d$-dimensional Brownian motion and $d$-dimensional Poisson process with intensity $\\lambda$ (in every component) consider an FBSDE:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t^i &= \\frac 1\\alpha \\exp\\left(-\\alpha X_t^i\\right) \\sum_j Z^{ij}_t dW^j_t - \\frac 1\\alpha \\sum_j \\log\\left(1+r^{ij}_t \\exp\\left(-\\alpha X^i_t\\right)\\right) dN^j_t & X_0 = x_0 \\\\\n",
    "dY_t^i &= \\frac 12 \\exp\\left(-\\alpha X^i_t\\right) \\sum_j \\left(Z^{ij}_t\\right)^2 dt + \\sum_j Z^{ij}_t dW^j_t + \\sum_j r^{ij}_t dN^j_t & Y^i_T= \\exp\\left(X^i_T\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "One can see by Ito's lemma (Oksendal, p.9, 1.2.8) that\n",
    "\n",
    "$$\n",
    "Y^i_t = \\exp(X^i_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c37f9f24-e9e1-4a9a-95e8-3fb2b0396ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working configuration without jumps: time_horizon = 0.1, n_paths = 2 ** 16, n_timesteps = 4, n_dimensions = 50\n",
    "# working configuration with jumps: time_horizon = 0.1, n_paths = 2 ** 12, n_timesteps = 4, n_dimensions = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26fff9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from makers.gpu_utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(pick_gpu_lowest_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ecf9b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate, Layer, BatchNormalization\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.metrics import mse\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4b8cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967ebe5",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "6c0b3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical parameters\n",
    "n_paths = 2 ** 12\n",
    "n_timesteps = 16\n",
    "\n",
    "# model parameters\n",
    "alpha = 1.\n",
    "time_horizon = 1.\n",
    "n_dimensions = 5\n",
    "n_diffusion_factors = 5\n",
    "n_jump_factors = 5\n",
    "intensity = 1.\n",
    "dt = time_horizon / n_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b6d03606-08d4-41e7-beb5-d5a6f3e002cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dirs\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"{timestamp}__{n_dimensions}__{n_paths}__{n_timesteps}__{time_horizon}\"\n",
    "tb_log_dir = \"/home/tmp/starokon/tensorboard/\" + model_name\n",
    "output_dir = f\"_output/models/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "d9b36d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z, r):\n",
    "    return [0. for _ in range(n_dimensions)]\n",
    "\n",
    "def s(t, x, y, z, r):\n",
    "    return [[tf.exp(-alpha * x[i]) * z[i][j] / alpha for j in range(n_diffusion_factors)] for i in range(n_dimensions)]\n",
    "\n",
    "def v(t, x, y, z, r):\n",
    "    return [[tf.math.log(1 + tf.math.maximum(r[i][j], 0.) * tf.exp(-alpha * x[i])) / alpha for j in range(n_jump_factors)] for i in range(n_dimensions)]\n",
    "\n",
    "def f(t, x, y, z, r):\n",
    "    return [tf.exp(-alpha * x[i]) * tf.reduce_sum(z[i] ** 2) / 2. for i in range(n_dimensions)]\n",
    "\n",
    "def g(x):\n",
    "    return [tf.exp(x[i]) for i in range(n_dimensions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120171a2",
   "metadata": {},
   "source": [
    "# Custom layers and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "750f491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialValue(Layer):\n",
    "    \n",
    "    def __init__(self, y0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.y0 = y0\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.y0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cece56b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "a7ab7b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(n_dimensions, n_diffusion_factors, n_jump_factors, n_timesteps, time_horizon):\n",
    "    \n",
    "    dt = time_horizon / n_timesteps\n",
    "    \n",
    "    def dX(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(b(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.tensordot(s(t, x, y, z, r), dW, [[1], [0]])\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.tensordot(v(t, x, y, z, r), dN, [[1], [0]])\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    def dY(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(f(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.tensordot(z, dW, [[1], [0]])\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.tensordot(r, dN, [[1], [0]])\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))        \n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    @tf.function\n",
    "    def hx(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return x + dX(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    @tf.function\n",
    "    def hy(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return y + dY(i * dt, x, y, z, r, dW, dN)\n",
    "    \n",
    "    paths = []\n",
    "\n",
    "    n_hidden_units = n_dimensions + n_diffusion_factors + n_jump_factors + 10\n",
    "\n",
    "    inputs_x0 = Input(shape=(n_dimensions))\n",
    "    inputs_dW = Input(shape=(n_timesteps, n_diffusion_factors))\n",
    "    inputs_dN = Input(shape=(n_timesteps, n_jump_factors))\n",
    "    \n",
    "    # variable x0\n",
    "\n",
    "    y0 = Dense(n_hidden_units, activation='elu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='y1_0')(inputs_x0)\n",
    "    y0 = Dense(n_dimensions, activation='elu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='y2_0')(y0)\n",
    "    \n",
    "    x = inputs_x0\n",
    "    y = y0\n",
    "\n",
    "    # constant x0\n",
    "    \n",
    "#     x0 = tf.Variable([[1. for _ in range(n_dimensions)]], trainable=False)\n",
    "#     y0 = tf.Variable([[0. for _ in range(n_dimensions)]], trainable=True)\n",
    "\n",
    "#     x = InitialValue(x0, trainable=False, name='x_0')(inputs_dW)\n",
    "#     y = InitialValue(y0, trainable=True, name='y_0')(inputs_dW)\n",
    "    \n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='z1_0')(z)\n",
    "    z = Dense(n_dimensions * n_diffusion_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='z2_0')(z)\n",
    "    # z = BatchNormalization(name='zbn_0')(z)\n",
    "    z = Reshape((n_dimensions, n_diffusion_factors), name='zr_0')(z)\n",
    "\n",
    "    r = concatenate([x, y])\n",
    "    r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='r1_0')(r)\n",
    "    r = Dense(n_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='r2_0')(r)\n",
    "    # r = BatchNormalization(name='rbn_0')(r)\n",
    "    r = Reshape((n_dimensions, n_jump_factors), name='rr_0')(r)\n",
    "\n",
    "    paths += [[x, y, z, r]]\n",
    "\n",
    "    # pre-compile lambda layers\n",
    "\n",
    "    for i in range(n_timesteps):\n",
    "\n",
    "        step = InitialValue(tf.Variable(i, dtype=tf.float32, trainable=False))(inputs_dW)\n",
    "\n",
    "        dW = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dW, step])\n",
    "        dN = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dN, step])\n",
    "\n",
    "        x, y = (\n",
    "            Lambda(hx, name=f'x_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "            Lambda(hy, name=f'y_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "        )\n",
    "\n",
    "        # we don't train z for the last time step; keep for consistency\n",
    "        z = concatenate([x, y])\n",
    "        z = Dense(n_hidden_units, activation='relu', name=f'z1_{i+1}')(z)\n",
    "        z = Dense(n_dimensions * n_diffusion_factors, activation='relu', name=f'z2_{i+1}')(z)\n",
    "        z = Reshape((n_dimensions, n_diffusion_factors), name=f'zr_{i+1}')(z)\n",
    "        # z = BatchNormalization(name=f'zbn_{i+1}')(z)\n",
    "\n",
    "        # we don't train r for the last time step; keep for consistency\n",
    "        r = concatenate([x, y])\n",
    "        r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name=f'r1_{i+1}')(r)\n",
    "        r = Dense(n_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name=f'r2_{i+1}')(r)\n",
    "        r = Reshape((n_dimensions, n_jump_factors), name=f'rr_{i+1}')(r)\n",
    "        # r = BatchNormalization(name=f'rbn_{i+1}')(r)\n",
    "\n",
    "        paths += [[x, y, z, r]]\n",
    "\n",
    "    outputs_loss = Lambda(lambda r: r[1] - tf.transpose(tf.vectorized_map(g, r[0])))([x, y])\n",
    "    outputs_paths = tf.stack(\n",
    "        [tf.stack([p[0] for p in paths[1:]], axis=1), tf.stack([p[1] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_diffusion_factors)] +\n",
    "        [tf.stack([p[3][:, :, i] for p in paths[1:]], axis=1) for i in range(n_jump_factors)], axis=2)\n",
    "\n",
    "\n",
    "    model_loss = Model([inputs_x0, inputs_dW, inputs_dN], outputs_loss)\n",
    "\n",
    "    # (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "    # skips the first time step\n",
    "    model_paths = Model([inputs_x0, inputs_dW, inputs_dN], outputs_paths)\n",
    "    \n",
    "    model_y0 = Model([inputs_x0], y0)\n",
    "\n",
    "    return model_loss, model_paths, model_y0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c335854-e926-4ca6-adf2-431f24826100",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "3a98d322-e9dd-484f-8e17-50056dd8a265",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=11, linewidth=90, formatter=dict(float=lambda x: \"%7.5g\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "d94a865b-56a6-4060-b168-5425f794bf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create model\n",
    "dt = time_horizon / n_timesteps\n",
    "model_loss, model_paths, model_y0 = build_model(n_dimensions=n_dimensions, n_diffusion_factors=n_dimensions, n_jump_factors=n_dimensions, n_timesteps=n_timesteps, time_horizon=time_horizon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "3abcc614-16c4-4acd-b286-7b53c4c7ffcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_loss.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "41902956",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x0 = tf.constant(np.full((n_paths, n_dimensions), 1.))\n",
    "x0 = 1. + 0. * tf.random.normal((n_paths, n_dimensions))\n",
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "dN = tf.random.poisson((n_paths, n_timesteps), tf.constant(dt * np.array([intensity for _ in range(n_jump_factors)])))\n",
    "target = tf.zeros((n_paths, n_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "b598fc4d-4ff4-470e-aba9-efce5ead3f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Y0Callback(Callback):\n",
    "    \n",
    "    def __init__(self, filepath=None):\n",
    "        super(Y0Callback, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.y0s = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y0 = model_y0(tf.constant([[1. for _ in range(n_dimensions)]]))\n",
    "        self.y0s += [y0[0]]\n",
    "        print(f\"{y0}\\n\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.filepath is not None:\n",
    "            pd.DataFrame(self.y0s).to_csv(self.filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "5d9c71ab-7290-4615-843f-88735a45f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define callbacks\n",
    "y0_callback = Y0Callback(filepath=os.path.join(output_dir, \"y0.csv\"))\n",
    "checkpoint_callback = ModelCheckpoint(os.path.join(output_dir, \"model.h5\"), monitor=\"loss\", save_weights_only=True, save_best_only=True, overwrite=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_log_dir, histogram_freq=1, profile_batch=1)\n",
    "nan_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "loss_callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=1e-4, patience=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "9eecc79e-8754-4400-8ada-437252964f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "callbacks = [\n",
    "    nan_callback,\n",
    "    checkpoint_callback,\n",
    "    # tensorboard_callback,\n",
    "    y0_callback,\n",
    "    loss_callback,\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "7b6de868-adb5-47db-afae-81af15bb1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave it here to readjust learning rate on the fly\n",
    "adam = Adam(learning_rate=1e-3) \n",
    "model_loss.compile(loss='mse', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2dbf6f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer InitialValue has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/1000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model_loss.fit([x0, dW, dN], target, batch_size=128, initial_epoch=0, epochs=1000, callbacks=[nan_callback, checkpoint_callback, tensorboard_callback, y0_callback, loss_callback])\n",
    "df_loss = pd.DataFrame(history.history['loss'])\n",
    "df_loss.to_csv(os.path.join(output_dir, 'loss.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ae6330-67ab-4aa8-9238-92e7bcc47db0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Transfer weights (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try transfer learning from another starting point\n",
    "\n",
    "model_loss.get_layer('y_0').set_weights(m_large.get_layer('y_0').get_weights())\n",
    "\n",
    "for i in range(n_timesteps):\n",
    "    model_loss.get_layer(f'z1_{i}').set_weights(m_large.get_layer(f'z1_{i}').get_weights())\n",
    "    model_loss.get_layer(f'z2_{i}').set_weights(m_large.get_layer(f'z2_{i}').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747f722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transfer learning from cruder discretization\n",
    "\n",
    "model_loss.get_layer('y_0').set_weights(m_small.get_layer('y_0').get_weights())\n",
    "\n",
    "n_small = 4\n",
    "\n",
    "for i in range(n_small):\n",
    "    for j in range(n_timesteps // n_small):\n",
    "        model_loss.get_layer(f'z1_{n_timesteps // n_small * i}').set_weights(m_small.get_layer(f'z1_{i}').get_weights())\n",
    "        model_loss.get_layer(f'z2_{n_timesteps // n_small * i}').set_weights(m_small.get_layer(f'z2_{i}').get_weights())\n",
    "        model_loss.get_layer(f'z1_{n_timesteps // n_small * i + j}').set_weights(m_small.get_layer(f'z1_{i}').get_weights())\n",
    "        model_loss.get_layer(f'z2_{n_timesteps // n_small * i + j}').set_weights(m_small.get_layer(f'z2_{i}').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34961895-e7be-41c6-bcb3-305c670f42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for exploding gradients before training\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = mse(model_loss([dW, dN]), target)\n",
    "\n",
    "# bias of the last dense layer\n",
    "variables = model_loss.variables[-1]\n",
    "tape.gradient(loss, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d368794-f5fc-419c-b35e-eea3f0d572e0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "85bc1ae9-42c8-44e4-a6ef-f774655eb159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (x_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (y_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "32/32 [==============================] - 9s 10ms/step - loss: 7.3783\n",
      "[0.032135 0.031664]\n",
      "\n",
      "Epoch 2/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 7.1694\n",
      "[0.064056 0.063324]\n",
      "\n",
      "Epoch 3/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 7.0043\n",
      "[ 0.0957 0.094791]\n",
      "\n",
      "Epoch 4/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 6.8362\n",
      "[0.12721 0.12611]\n",
      "\n",
      "Epoch 5/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 6.6648\n",
      "[0.15838 0.15734]\n",
      "\n",
      "Epoch 6/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 6.5089\n",
      "[0.18953 0.18835]\n",
      "\n",
      "Epoch 7/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 6.3461\n",
      "[0.22038 0.21921]\n",
      "\n",
      "Epoch 8/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 6.1820\n",
      "[0.25115 0.24986]\n",
      "\n",
      "Epoch 9/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 6.0358\n",
      "[0.28155 0.28035]\n",
      "\n",
      "Epoch 10/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 5.8780\n",
      "[0.31192 0.31062]\n",
      "\n",
      "Epoch 11/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 5.7410\n",
      "[0.34202 0.34074]\n",
      "\n",
      "Epoch 12/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 5.5909\n",
      "[0.37199 0.37066]\n",
      "\n",
      "Epoch 13/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 5.4635\n",
      "[0.40172 0.40047]\n",
      "\n",
      "Epoch 14/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 5.3155\n",
      "[0.43131 0.43002]\n",
      "\n",
      "Epoch 15/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 5.1749\n",
      "[ 0.4608 0.45933]\n",
      "\n",
      "Epoch 16/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 5.0454\n",
      "[0.48998 0.48857]\n",
      "\n",
      "Epoch 17/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.9068\n",
      "[0.51899 0.51758]\n",
      "\n",
      "Epoch 18/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 4.7979\n",
      "[0.54786 0.54646]\n",
      "\n",
      "Epoch 19/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 4.6504\n",
      "[ 0.5766 0.57502]\n",
      "\n",
      "Epoch 20/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 4.5185\n",
      "[0.60502 0.60355]\n",
      "\n",
      "Epoch 21/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.4111\n",
      "[0.63337 0.63182]\n",
      "\n",
      "Epoch 22/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.2932\n",
      "[0.66145 0.65998]\n",
      "\n",
      "Epoch 23/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.1763\n",
      "[0.68952 0.68784]\n",
      "\n",
      "Epoch 24/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 4.0717\n",
      "[0.71714 0.71577]\n",
      "\n",
      "Epoch 25/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 3.9533\n",
      "[ 0.7448  0.7433]\n",
      "\n",
      "Epoch 26/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 3.8471\n",
      "[0.77232 0.77064]\n",
      "\n",
      "Epoch 27/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 3.7342\n",
      "[0.79962  0.7979]\n",
      "\n",
      "Epoch 28/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 3.6300\n",
      "[0.82675 0.82492]\n",
      "\n",
      "Epoch 29/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 3.5352\n",
      "[0.85362 0.85184]\n",
      "\n",
      "Epoch 30/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 3.4185\n",
      "[0.88037 0.87843]\n",
      "\n",
      "Epoch 31/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 3.3331\n",
      "[0.90691 0.90498]\n",
      "\n",
      "Epoch 32/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 3.2316\n",
      "[0.93325 0.93136]\n",
      "\n",
      "Epoch 33/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 3.1467\n",
      "[ 0.9595 0.95753]\n",
      "\n",
      "Epoch 34/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 3.0456\n",
      "[0.98548 0.98358]\n",
      "\n",
      "Epoch 35/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 2.9553\n",
      "[ 1.0113  1.0094]\n",
      "\n",
      "Epoch 36/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 2.8611\n",
      "[  1.037   1.035]\n",
      "\n",
      "Epoch 37/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 2.7931\n",
      "[ 1.0626  1.0606]\n",
      "\n",
      "Epoch 38/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 2.7034\n",
      "[ 1.0878  1.0859]\n",
      "\n",
      "Epoch 39/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.6067\n",
      "[ 1.1131   1.111]\n",
      "\n",
      "Epoch 40/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 2.5323\n",
      "[  1.138   1.136]\n",
      "\n",
      "Epoch 41/3000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 2.4611\n",
      "[ 1.1629  1.1607]\n",
      "\n",
      "Epoch 42/3000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 2.3757\n",
      "[ 1.1875  1.1854]\n",
      "\n",
      "Epoch 43/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.3126\n",
      "[  1.212  1.2098]\n",
      "\n",
      "Epoch 44/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.2414\n",
      "[ 1.2362  1.2341]\n",
      "\n",
      "Epoch 45/3000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 2.1640\n",
      "[ 1.2603  1.2582]\n",
      "\n",
      "Epoch 46/3000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 2.1011\n",
      "[ 1.2839  1.2826]\n",
      "\n",
      "Epoch 47/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 2.0329\n",
      "[ 1.3077  1.3063]\n",
      "\n",
      "Epoch 48/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 1.9647\n",
      "[ 1.3314  1.3298]\n",
      "\n",
      "Epoch 49/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 1.9007\n",
      "[ 1.3549  1.3531]\n",
      "\n",
      "Epoch 50/3000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 1.8305\n",
      "[ 1.3782  1.3762]\n",
      "\n",
      "Epoch 51/3000\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 1.7694\n",
      "[ 1.4013  1.3991]\n",
      "\n",
      "Epoch 52/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 1.7013\n",
      "[ 1.4242  1.4219]\n",
      "\n",
      "Epoch 53/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 1.6557\n",
      "[ 1.4469  1.4446]\n",
      "\n",
      "Epoch 54/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 1.5815\n",
      "[ 1.4696  1.4669]\n",
      "\n",
      "Epoch 55/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 1.5349\n",
      "[ 1.4915  1.4897]\n",
      "\n",
      "Epoch 56/3000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 1.4828\n",
      "[ 1.5138  1.5117]\n",
      "\n",
      "Epoch 57/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 1.4318\n",
      "[ 1.5358  1.5336]\n",
      "\n",
      "Epoch 58/3000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 1.3770\n",
      "[ 1.5576  1.5554]\n",
      "\n",
      "Epoch 59/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 1.3242\n",
      "[ 1.5791   1.577]\n",
      "\n",
      "Epoch 60/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 1.2824\n",
      "[ 1.6006  1.5985]\n",
      "\n",
      "Epoch 61/3000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 1.2244\n",
      "[ 1.6219  1.6195]\n",
      "\n",
      "Epoch 62/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 1.1896\n",
      "[ 1.6429  1.6407]\n",
      "\n",
      "Epoch 63/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 1.1418\n",
      "[ 1.6637  1.6616]\n",
      "\n",
      "Epoch 64/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 1.0985\n",
      "[ 1.6845  1.6822]\n",
      "\n",
      "Epoch 65/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 1.0540\n",
      "[  1.705  1.7027]\n",
      "\n",
      "Epoch 66/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 1.0130\n",
      "[ 1.7252  1.7231]\n",
      "\n",
      "Epoch 67/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.9689\n",
      "[ 1.7453  1.7432]\n",
      "\n",
      "Epoch 68/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.9341\n",
      "[ 1.7652  1.7632]\n",
      "\n",
      "Epoch 69/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8999\n",
      "[ 1.7851  1.7828]\n",
      "\n",
      "Epoch 70/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8549\n",
      "[ 1.8047  1.8022]\n",
      "\n",
      "Epoch 71/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.8260\n",
      "[ 1.8239  1.8217]\n",
      "\n",
      "Epoch 72/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7865\n",
      "[ 1.8431  1.8409]\n",
      "\n",
      "Epoch 73/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7588\n",
      "[ 1.8621  1.8598]\n",
      "\n",
      "Epoch 74/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.7176\n",
      "[ 1.8808  1.8786]\n",
      "\n",
      "Epoch 75/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.6858\n",
      "[ 1.8994  1.8972]\n",
      "\n",
      "Epoch 76/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.6627\n",
      "[ 1.9176  1.9157]\n",
      "\n",
      "Epoch 77/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.6324\n",
      "[ 1.9358  1.9339]\n",
      "\n",
      "Epoch 78/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.5993\n",
      "[ 1.9539  1.9517]\n",
      "\n",
      "Epoch 79/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5732\n",
      "[ 1.9714  1.9698]\n",
      "\n",
      "Epoch 80/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.5500\n",
      "[ 1.9891  1.9873]\n",
      "\n",
      "Epoch 81/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.5273\n",
      "[ 2.0066  2.0046]\n",
      "\n",
      "Epoch 82/3000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.5042\n",
      "[ 2.0239  2.0217]\n",
      "\n",
      "Epoch 83/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.4724\n",
      "[ 2.0407  2.0386]\n",
      "\n",
      "Epoch 84/3000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.4527\n",
      "[ 2.0576  2.0553]\n",
      "\n",
      "Epoch 85/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.4271\n",
      "[  2.074   2.072]\n",
      "\n",
      "Epoch 86/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.4100\n",
      "[ 2.0903  2.0882]\n",
      "\n",
      "Epoch 87/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.3882\n",
      "[ 2.1065  2.1044]\n",
      "\n",
      "Epoch 88/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.3705\n",
      "[ 2.1224  2.1203]\n",
      "\n",
      "Epoch 89/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.3508\n",
      "[ 2.1382  2.1359]\n",
      "\n",
      "Epoch 90/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.3343\n",
      "[ 2.1537  2.1514]\n",
      "\n",
      "Epoch 91/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.3128\n",
      "[ 2.1689  2.1667]\n",
      "\n",
      "Epoch 92/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.2969\n",
      "[  2.184  2.1818]\n",
      "\n",
      "Epoch 93/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.2813\n",
      "[ 2.1988  2.1966]\n",
      "\n",
      "Epoch 94/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.2647\n",
      "[ 2.2134  2.2113]\n",
      "\n",
      "Epoch 95/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.2521\n",
      "[ 2.2277  2.2258]\n",
      "\n",
      "Epoch 96/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.2384\n",
      "[  2.242  2.2399]\n",
      "\n",
      "Epoch 97/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.2240\n",
      "[ 2.2559  2.2539]\n",
      "\n",
      "Epoch 98/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.2105\n",
      "[ 2.2696  2.2677]\n",
      "\n",
      "Epoch 99/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.1992\n",
      "[ 2.2832  2.2812]\n",
      "\n",
      "Epoch 100/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.1853\n",
      "[ 2.2965  2.2945]\n",
      "\n",
      "Epoch 101/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.1752\n",
      "[ 2.3095  2.3076]\n",
      "\n",
      "Epoch 102/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.1645\n",
      "[ 2.3224  2.3204]\n",
      "\n",
      "Epoch 103/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.1529\n",
      "[  2.335  2.3331]\n",
      "\n",
      "Epoch 104/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.1438\n",
      "[ 2.3473  2.3454]\n",
      "\n",
      "Epoch 105/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.1361\n",
      "[ 2.3594  2.3577]\n",
      "\n",
      "Epoch 106/3000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.1273\n",
      "[ 2.3713  2.3696]\n",
      "\n",
      "Epoch 107/3000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.1188\n",
      "[  2.383  2.3813]\n",
      "\n",
      "Epoch 108/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.1114\n",
      "[ 2.3945  2.3928]\n",
      "\n",
      "Epoch 109/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.1037\n",
      "[ 2.4057   2.404]\n",
      "\n",
      "Epoch 110/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0968\n",
      "[ 2.4167   2.415]\n",
      "\n",
      "Epoch 111/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0896\n",
      "[ 2.4274  2.4259]\n",
      "\n",
      "Epoch 112/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0829\n",
      "[ 2.4379  2.4364]\n",
      "\n",
      "Epoch 113/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0775\n",
      "[ 2.4483  2.4467]\n",
      "\n",
      "Epoch 114/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0720\n",
      "[ 2.4583  2.4567]\n",
      "\n",
      "Epoch 115/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0666\n",
      "[ 2.4681  2.4666]\n",
      "\n",
      "Epoch 116/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0615\n",
      "[ 2.4777  2.4762]\n",
      "\n",
      "Epoch 117/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0571\n",
      "[ 2.4871  2.4857]\n",
      "\n",
      "Epoch 118/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0529\n",
      "[ 2.4962  2.4948]\n",
      "\n",
      "Epoch 119/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 0.0495\n",
      "[ 2.5051  2.5037]\n",
      "\n",
      "Epoch 120/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0448\n",
      "[ 2.5138  2.5124]\n",
      "\n",
      "Epoch 121/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0412\n",
      "[ 2.5223  2.5209]\n",
      "\n",
      "Epoch 122/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0382\n",
      "[ 2.5305  2.5291]\n",
      "\n",
      "Epoch 123/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0349\n",
      "[ 2.5385  2.5371]\n",
      "\n",
      "Epoch 124/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0323\n",
      "[ 2.5463  2.5449]\n",
      "\n",
      "Epoch 125/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0299\n",
      "[ 2.5538  2.5525]\n",
      "\n",
      "Epoch 126/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0268\n",
      "[ 2.5611  2.5598]\n",
      "\n",
      "Epoch 127/3000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0249\n",
      "[ 2.5682   2.567]\n",
      "\n",
      "Epoch 128/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0224\n",
      "[ 2.5751  2.5739]\n",
      "\n",
      "Epoch 129/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0203\n",
      "[ 2.5818  2.5806]\n",
      "\n",
      "Epoch 130/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0185\n",
      "[ 2.5883  2.5871]\n",
      "\n",
      "Epoch 131/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0168\n",
      "[ 2.5946  2.5934]\n",
      "\n",
      "Epoch 132/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0152\n",
      "[ 2.6006  2.5995]\n",
      "\n",
      "Epoch 133/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0138\n",
      "[ 2.6065  2.6054]\n",
      "\n",
      "Epoch 134/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0126\n",
      "[ 2.6121   2.611]\n",
      "\n",
      "Epoch 135/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 0.0114\n",
      "[ 2.6176  2.6165]\n",
      "\n",
      "Epoch 136/3000\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 0.0102\n",
      "[ 2.6227  2.6218]\n",
      "\n",
      "Epoch 137/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0093\n",
      "[ 2.6278  2.6268]\n",
      "\n",
      "Epoch 138/3000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.0083\n",
      "[ 2.6326  2.6317]\n",
      "\n",
      "Epoch 139/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.0075\n",
      "[ 2.6373  2.6364]\n",
      "\n",
      "Epoch 140/3000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.0067\n",
      "[ 2.6418  2.6409]\n",
      "\n",
      "Epoch 141/3000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 0.0060\n",
      "[ 2.6461  2.6453]\n",
      "\n",
      "Epoch 142/3000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 0.0055\n",
      "[ 2.6501  2.6494]\n",
      "\n",
      "Epoch 143/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.0049\n",
      "[ 2.6541  2.6534]\n",
      "\n",
      "Epoch 144/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.0044\n",
      "[ 2.6579  2.6572]\n",
      "\n",
      "Epoch 145/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.0039\n",
      "[ 2.6615  2.6608]\n",
      "\n",
      "Epoch 146/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.0035\n",
      "[  2.665  2.6643]\n",
      "\n",
      "Epoch 147/3000\n",
      "32/32 [==============================] - 0s 11ms/step - loss: 0.0031\n",
      "[ 2.6683  2.6676]\n",
      "\n",
      "Epoch 148/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.0027\n",
      "[ 2.6714  2.6707]\n",
      "\n",
      "Epoch 149/3000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 0.0024\n",
      "[ 2.6744  2.6737]\n",
      "\n",
      "Epoch 150/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.0022\n",
      "[ 2.6772  2.6766]\n",
      "\n",
      "Epoch 151/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.0019\n",
      "[ 2.6799  2.6793]\n",
      "\n",
      "Epoch 152/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 0.0017\n",
      "[ 2.6824  2.6819]\n",
      "\n",
      "Epoch 153/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 0.0016\n",
      "[ 2.6848  2.6844]\n",
      "\n",
      "Epoch 154/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.0014\n",
      "[ 2.6872  2.6867]\n",
      "\n",
      "Epoch 155/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.0013\n",
      "[ 2.6893  2.6888]\n",
      "\n",
      "Epoch 156/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 0.0011\n",
      "[ 2.6913  2.6909]\n",
      "\n",
      "Epoch 157/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 9.9248e-04\n",
      "[ 2.6933  2.6929]\n",
      "\n",
      "Epoch 158/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 8.9105e-04\n",
      "[ 2.6951  2.6947]\n",
      "\n",
      "Epoch 159/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 7.9030e-04\n",
      "[ 2.6968  2.6965]\n",
      "\n",
      "Epoch 160/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 7.4469e-04\n",
      "[ 2.6984  2.6981]\n",
      "\n",
      "Epoch 161/3000\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 7.1879e-04\n",
      "[ 2.6999  2.6996]\n",
      "\n",
      "Epoch 162/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 7.1370e-04\n",
      "[ 2.7013   2.701]\n",
      "\n",
      "Epoch 163/3000\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 6.4608e-04\n",
      "[ 2.7027  2.7023]\n",
      "\n",
      "Epoch 164/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 4.7986e-04\n",
      "[ 2.7039  2.7036]\n",
      "\n",
      "Epoch 165/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 4.6773e-04\n",
      "[ 2.7051  2.7048]\n",
      "\n",
      "Epoch 166/3000\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 4.3421e-04\n",
      "[ 2.7062  2.7059]\n",
      "\n",
      "Epoch 167/3000\n",
      "32/32 [==============================] - 0s 10ms/step - loss: 3.8342e-04\n",
      "[ 2.7072  2.7069]\n",
      "\n",
      "Epoch 168/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 3.9288e-04\n",
      "[ 2.7081  2.7078]\n",
      "\n",
      "Epoch 169/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 3.5403e-04\n",
      "[  2.709  2.7087]\n",
      "\n",
      "Epoch 170/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 3.3383e-04\n",
      "[ 2.7098  2.7095]\n",
      "\n",
      "Epoch 171/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.9650e-04\n",
      "[ 2.7105  2.7103]\n",
      "\n",
      "Epoch 172/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 3.6526e-04\n",
      "[ 2.7112   2.711]\n",
      "\n",
      "Epoch 173/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 9.3501e-04\n",
      "[ 2.7118  2.7116]\n",
      "\n",
      "Epoch 174/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 8.5153e-04\n",
      "[ 2.7124  2.7122]\n",
      "\n",
      "Epoch 175/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 4.1769e-04\n",
      "[ 2.7129  2.7128]\n",
      "\n",
      "Epoch 176/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.5550e-04\n",
      "[ 2.7134  2.7133]\n",
      "\n",
      "Epoch 177/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.2769e-04\n",
      "[ 2.7139  2.7137]\n",
      "\n",
      "Epoch 178/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 2.6245e-04\n",
      "[ 2.7143  2.7142]\n",
      "\n",
      "Epoch 179/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.2588e-04\n",
      "[ 2.7147  2.7146]\n",
      "\n",
      "Epoch 180/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.2416e-04\n",
      "[  2.715  2.7149]\n",
      "\n",
      "Epoch 181/3000\n",
      "32/32 [==============================] - 0s 13ms/step - loss: 2.1482e-04\n",
      "[ 2.7153  2.7152]\n",
      "\n",
      "Epoch 182/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 2.3355e-04\n",
      "[ 2.7156  2.7156]\n",
      "\n",
      "Epoch 183/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 2.0153e-04\n",
      "[ 2.7158  2.7158]\n",
      "\n",
      "Epoch 184/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 2.1286e-04\n",
      "[ 2.7161   2.716]\n",
      "\n",
      "Epoch 185/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 2.0595e-04\n",
      "[ 2.7163  2.7163]\n",
      "\n",
      "Epoch 186/3000\n",
      "32/32 [==============================] - 0s 16ms/step - loss: 2.0008e-04\n",
      "[ 2.7165  2.7165]\n",
      "\n",
      "Epoch 187/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 1.8426e-04\n",
      "[ 2.7167  2.7166]\n",
      "\n",
      "Epoch 188/3000\n",
      "32/32 [==============================] - 0s 12ms/step - loss: 1.8423e-04\n",
      "[ 2.7169  2.7168]\n",
      "\n",
      "Epoch 189/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.0285e-04\n",
      "[  2.717   2.717]\n",
      "\n",
      "Epoch 190/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 1.6246e-04\n",
      "[ 2.7171  2.7171]\n",
      "\n",
      "Epoch 191/3000\n",
      "32/32 [==============================] - 0s 14ms/step - loss: 1.7185e-04\n",
      "[ 2.7172  2.7172]\n",
      "\n",
      "Epoch 192/3000\n",
      "32/32 [==============================] - 0s 15ms/step - loss: 1.6426e-04\n",
      "[ 2.7173  2.7173]\n",
      "\n",
      "Epoch 193/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 1.6121e-04\n",
      "[ 2.7174  2.7174]\n",
      "\n",
      "Epoch 194/3000\n",
      "32/32 [==============================] - 1s 16ms/step - loss: 1.7821e-04\n",
      "[ 2.7175  2.7175]\n",
      "\n",
      "Epoch 195/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 2.1643e-04\n",
      "[ 2.7176  2.7175]\n",
      "\n",
      "Epoch 196/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 1.6202e-04\n",
      "[ 2.7177  2.7176]\n",
      "\n",
      "Epoch 197/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 1.4946e-04\n",
      "[ 2.7177  2.7177]\n",
      "\n",
      "Epoch 198/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 1.7086e-04\n",
      "[ 2.7178  2.7177]\n",
      "\n",
      "Epoch 199/3000\n",
      "32/32 [==============================] - 0s 9ms/step - loss: 1.7356e-04\n",
      "[ 2.7178  2.7177]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [01:42<00:00, 102.88s/it]\n"
     ]
    }
   ],
   "source": [
    "for time_horizon, n_paths, n_timesteps, n_dimensions in tqdm(list(product([0.1], [2**12], [4], [2]))):\n",
    "    \n",
    "    # create model\n",
    "    dt = time_horizon / n_timesteps\n",
    "    model_loss, model_paths = build_model(n_dimensions=n_dimensions, n_diffusion_factors=n_dimensions, n_jump_factors=n_dimensions, n_timesteps=n_timesteps, time_horizon=time_horizon)\n",
    "    \n",
    "    # generate paths\n",
    "    dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "    dN = tf.random.poisson((n_paths, n_timesteps), tf.constant(dt * np.array([intensity for _ in range(n_jump_factors)]).transpose().reshape(-1)))\n",
    "    target = tf.zeros((n_paths, n_dimensions))\n",
    "    \n",
    "    # create dirs\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model_name = f\"{timestamp}__{n_dimensions}__{n_paths}__{n_timesteps}__{time_horizon}\"\n",
    "    tb_log_dir = \"/home/tmp/starokon/tensorboard/\" + model_name\n",
    "    output_dir = f\"_output/models/{model_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # define callbacks\n",
    "    y0_callback = Y0Callback(filepath=os.path.join(output_dir, \"y0.csv\"))\n",
    "    checkpoint_callback = ModelCheckpoint(os.path.join(output_dir, \"model.h5\"), monitor=\"loss\", save_weights_only=True, save_best_only=True, overwrite=True)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_log_dir, histogram_freq=1, profile_batch=1)\n",
    "    nan_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "    loss_callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=1e-4, patience=20)\n",
    "    \n",
    "    # callbacks\n",
    "    callbacks = [\n",
    "        nan_callback,\n",
    "        checkpoint_callback,\n",
    "        # tensorboard_callback,\n",
    "        y0_callback,\n",
    "        loss_callback,\n",
    "    ]\n",
    "    \n",
    "    # leave it here to readjust learning rate on the fly\n",
    "    adam = Adam(learning_rate=1e-3) \n",
    "    model_loss.compile(loss='mse', optimizer=adam)\n",
    "    \n",
    "    # train\n",
    "    history = model_loss.fit([dW, dN], target, batch_size=128, initial_epoch=0, epochs=3000, callbacks=callbacks)\n",
    "    df_loss = pd.DataFrame(history.history['loss'])\n",
    "    df_loss.to_csv(os.path.join(output_dir, 'loss.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d203e6-7d4f-4cb6-bd0f-23ad8361c0b5",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e3f366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 4s 2ms/step - loss: 3.4178e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.417778726699794e-09"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW_test = tf.sqrt(dt) * tf.random.normal((n_paths//8, n_timesteps, n_diffusion_factors))\n",
    "dN_test = tf.random.poisson((n_paths//8, n_timesteps), tf.constant(dt * np.array([1., 1.]).transpose().reshape(-1)))\n",
    "target_test = tf.zeros((n_paths//8, n_dimensions))\n",
    "\n",
    "model_loss.evaluate([dW_test, dN_test], target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba9886",
   "metadata": {},
   "source": [
    "# Display paths and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bad model\n",
    "model_loss.load_weights('_models/weights0011.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e765bc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.21302719e-03, -1.83777374e-04,  7.68974721e-02, ...,\n",
       "        -1.08535380e-04, -3.65010686e-02, -1.43885612e-04],\n",
       "       [ 3.12728644e-03, -1.86214020e-04,  1.65139437e-02, ...,\n",
       "        -1.07569365e-04,  9.47822630e-02, -1.43885612e-04],\n",
       "       [-1.13846944e-03, -1.84155477e-04,  3.47495675e-02, ...,\n",
       "        -1.07487824e-04, -5.74785471e-03, -1.43885612e-04],\n",
       "       ...,\n",
       "       [-4.19508014e-03, -1.82968608e-04,  9.62089002e-03, ...,\n",
       "        -1.07971777e-04, -5.63402846e-03, -1.43885612e-04],\n",
       "       [-1.20397564e-02, -1.82203512e-04,  1.12469167e-01, ...,\n",
       "        -1.08954977e-04, -2.62902267e-02, -1.43885612e-04],\n",
       "       [-3.71439802e-03, -1.83611395e-04,  4.11325842e-02, ...,\n",
       "        -1.07773740e-04,  1.11734495e-02, -1.43885612e-04]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model_loss([dW, dN]).numpy()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f074bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = model_paths([dW, dN]).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
