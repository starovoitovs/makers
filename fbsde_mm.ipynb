{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7adda584-f8d0-4475-8031-ef560ec0eae1",
   "metadata": {},
   "source": [
    "# FBSDE\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a810fbc7-9c2f-41fd-83b9-cc3987654ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55ed80e7-951e-467c-9d2f-ae92e6453acf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-17 19:19:01.385130: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2022-03-17 19:19:01.569975: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:03:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2022-03-17 19:19:01.571665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:44:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2022-03-17 19:19:01.573331: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 2 with properties: \n",
      "pciBusID: 0000:84:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2022-03-17 19:19:01.574992: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 3 with properties: \n",
      "pciBusID: 0000:c4:00.0 name: NVIDIA A100-SXM4-40GB computeCapability: 8.0\n",
      "coreClock: 1.41GHz coreCount: 108 deviceMemorySize: 39.59GiB deviceMemoryBandwidth: 1.41TiB/s\n",
      "2022-03-17 19:19:01.575018: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2022-03-17 19:19:02.110676: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2022-03-17 19:19:02.110721: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2022-03-17 19:19:02.245590: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2022-03-17 19:19:02.384673: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2022-03-17 19:19:02.489322: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2022-03-17 19:19:02.587474: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2022-03-17 19:19:02.602686: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2022-03-17 19:19:02.616282: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1, 2, 3\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ead24874-5164-405d-bf64-f13d7c6e8511",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_paths = 256\n",
    "n_timesteps = 10\n",
    "n_dimensions = 4\n",
    "n_factors = 2\n",
    "T = 1.\n",
    "dt = T / n_timesteps\n",
    "batch_size = 128\n",
    "epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "315af737-b3d1-4196-a36d-d82d67ef2ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convention: x = (s, alpha, q, c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "139ef913-e310-4583-b090-4826fd30d524",
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1.\n",
    "lp = 1.\n",
    "lm = 1.\n",
    "k = 100.\n",
    "sigma = 1.\n",
    "zeta = 1.\n",
    "phi = 1.\n",
    "psi = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ed6c3c18-cbb2-400a-9f4b-02bac81a1341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z):\n",
    "    return [\n",
    "        x[1],\n",
    "        -eta * x[0],\n",
    "        lp * tf.exp(-1 + k * y[2] / y[3] + x[0] * k) - lm * tf.exp(-1 - k * y[2] / y[3] - x[0] * k),\n",
    "        lp * (1./k - y[2] / y[3]) * tf.exp(-1 + k * y[2] / y[3] + x[0] * k) - lm * (-1./k - y[2] / y[3]) * tf.exp(-1. - k * y[2] / y[3] - x[0] * k),\n",
    "    ]\n",
    "\n",
    "def s(t, x, y, z):\n",
    "    return [[sigma, 0], [0, zeta], [0, 0], [0, 0]]\n",
    "\n",
    "def dH_dx(t, x, y, z):\n",
    "    return [\n",
    "        y[3] * lp * tf.exp(-1. + k * y[2] / y[3] + x[0] * k) - y[3] * lm * tf.exp(-1. - k * y[2] / y[3] - x[0] * k),\n",
    "        y[0] - eta * y[1],\n",
    "        -2. * phi * x[2],\n",
    "        0.\n",
    "    ]\n",
    "\n",
    "def dg_dx(x):\n",
    "    return [x[2], 0., x[0] - 2 * psi * x[2], 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "c2ebbb08-c357-4d31-bd93-35074b4f7fe0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_399), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 4) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (lambda_400), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 4) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "\n",
    "inputs_dW = Input(shape=(n_timesteps, n_factors))\n",
    "\n",
    "x0 = tf.constant([[0., 0., 0., 0.]])\n",
    "y0 = tf.Variable([[10., 10., 10., 10.]])\n",
    "\n",
    "x = x0\n",
    "y = y0\n",
    "\n",
    "z = concatenate([x, y])\n",
    "z = Dense(10, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-2))(z)\n",
    "z = Dense(n_dimensions * n_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-2))(z)\n",
    "z = Reshape((n_dimensions, n_factors))(z)\n",
    "\n",
    "paths += [[x, y, z]]\n",
    "\n",
    "for i in range(n_timesteps):\n",
    "            \n",
    "    def dX(x, y, z, dw):\n",
    "        \n",
    "        def drift(arg):\n",
    "            x, y, z = arg\n",
    "            return tf.math.multiply(b(i*dt, x, y, z), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, dw = arg\n",
    "            return tf.tensordot(s(i*dt, x, y, z), dw[i], [[1], [0]])\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, dw))\n",
    "        \n",
    "        return a0 + a1\n",
    "\n",
    "    def dY(x, y, z, dw):\n",
    "        \n",
    "        def drift(arg):\n",
    "            x, y, z = arg\n",
    "            return tf.math.multiply(dH_dx(i*dt, x, y, z), -dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, dw = arg\n",
    "            return tf.tensordot(z, dw[i], [[1], [0]])\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, dw))\n",
    "        \n",
    "        return a0 + a1\n",
    "    \n",
    "    x, y = (\n",
    "        Lambda(lambda r: r[0] + dX(r[0], r[1], r[2], r[3]))([x, y, z, inputs_dW]),\n",
    "        Lambda(lambda r: r[1] + dY(r[0], r[1], r[2], r[3]))([x, y, z, inputs_dW]),\n",
    "    )\n",
    "    \n",
    "    # we don't train z for the last time step; keep for consistency\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(10, activation='relu')(z)\n",
    "    z = Dense(n_dimensions * n_factors, activation='relu')(z)\n",
    "    z = Reshape((n_dimensions, n_factors))(z)\n",
    "\n",
    "    paths += [[x, y, z]]\n",
    "    \n",
    "outputs_loss = Lambda(lambda r: r[1] - tf.transpose(tf.vectorized_map(dg_dx, r[0])))([x, y])\n",
    "model_loss = Model(inputs_dW, outputs_loss)\n",
    "model_loss.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "# skips the first time step\n",
    "outputs_paths = tf.stack([tf.stack([p[0] for p in paths[1:]], axis=1), tf.stack([p[1] for p in paths[1:]], axis=1)] + [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_factors)], axis=2)\n",
    "model_paths = Model(inputs_dW, outputs_paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf827ce-a9b3-4f56-bf18-6bf78e9f9fe2",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a974bef6-b86d-4fd0-8ce5-2639da0e7f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:`period` argument is deprecated. Please use `save_freq` to specify the frequency in number of batches seen.\n",
      "Epoch 1/100\n",
      "2/2 [==============================] - 59s 12ms/step - loss: nan\n",
      "Epoch 2/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 3/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 4/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 5/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 6/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 7/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 8/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 9/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 10/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 11/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 12/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 13/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 14/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 15/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 16/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 17/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 18/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 19/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 20/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 21/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 22/100\n",
      "2/2 [==============================] - 0s 13ms/step - loss: nan\n",
      "Epoch 23/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 24/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 25/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 26/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 27/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 28/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 29/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 30/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 31/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 32/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 33/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 34/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 35/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 36/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 37/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 38/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 39/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 40/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 41/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 42/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 43/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 44/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 45/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 46/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 47/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 48/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 49/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 50/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 51/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 52/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 53/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 54/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 55/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 56/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 57/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 58/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 59/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 60/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 61/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 62/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 63/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 64/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 65/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 66/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 67/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 68/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 69/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 70/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 71/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 72/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 73/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 74/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 75/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 76/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 77/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 78/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 79/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 80/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 81/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 82/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 83/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n",
      "Epoch 84/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 85/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 86/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 87/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 88/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 89/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 90/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 91/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 92/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 93/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 94/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 95/100\n",
      "2/2 [==============================] - 0s 12ms/step - loss: nan\n",
      "Epoch 96/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 97/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 98/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 99/100\n",
      "2/2 [==============================] - 0s 11ms/step - loss: nan\n",
      "Epoch 100/100\n",
      "2/2 [==============================] - 0s 10ms/step - loss: nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x153d8a1b25e0>"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_factors))\n",
    "target = tf.zeros((n_paths, 4))\n",
    "callback = ModelCheckpoint('_models/weights{epoch:03d}.h5', period=1, save_weights_only=True, overwrite=True)\n",
    "model_loss.fit(dW, target, batch_size=batch_size, epochs=epochs, callbacks=[callback])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b011bd1d-4eb1-4671-b023-f3739ae1ce35",
   "metadata": {},
   "source": [
    "# Display paths and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "9de261bb-e813-4b2e-8b7c-be79dd22af9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bad model\n",
    "model_loss.load_weights('_models/weights001.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "84eae2e4-a26c-415b-b894-3c9b38534341",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       ...,\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan],\n",
       "       [      nan,       nan,       nan,       nan]], dtype=float32)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model_loss(dW).numpy()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "bf5bf926-5b40-4d81-9431-aae2ee7ea622",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = model_paths(dW).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "1127c7ce-db2a-482f-87f4-c2e0601cc88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[   0.0298,   0.06413,    0.1027,    0.1452,    0.1911,    0.2402,    0.2918,    0.3456,     0.401,    0.4575],\n",
       "        [  0.04532,   0.08766,    0.1266,    0.1616,    0.1924,    0.2186,    0.2399,    0.2561,    0.2668,     0.272],\n",
       "        [      inf,       inf,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan],\n",
       "        [     -inf,      -inf,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan]],\n",
       "\n",
       "       [[     -inf,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan],\n",
       "        [       10,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan],\n",
       "        [       10,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan],\n",
       "        [       10,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan]],\n",
       "\n",
       "       [[      nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan],\n",
       "        [      nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan],\n",
       "        [      nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan],\n",
       "        [      nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan]],\n",
       "\n",
       "       [[      nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan],\n",
       "        [      nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan],\n",
       "        [      nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan],\n",
       "        [      nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan,       nan]]], dtype=float32)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%9.4g\" % x))\n",
    "tf.transpose(paths[112, :, :, :], (1, 2, 0)).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
