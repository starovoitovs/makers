{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc06f6d1",
   "metadata": {},
   "source": [
    "# FBSDE\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a52e06e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate, Layer\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea6fecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  3\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "dd71b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical parameters\n",
    "n_paths = 2 ** 20\n",
    "n_timesteps = 4\n",
    "n_dimensions = 4\n",
    "n_diffusion_factors = 2\n",
    "n_jump_factors = 2\n",
    "T = 1.\n",
    "dt = T / n_timesteps\n",
    "batch_size = 128\n",
    "epochs = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "02c1be8d-1b2f-42b3-a12b-57e78ab28782",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "eta = 1.\n",
    "lp = 1.\n",
    "lm = 1.\n",
    "k = 100.\n",
    "sigma = 1.\n",
    "zeta = 1.\n",
    "phi = 1.\n",
    "psi = 1.\n",
    "epsilon = 5e-3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f1815e-1461-4872-8c35-408cf1ad7d89",
   "metadata": {},
   "source": [
    "# Initial value layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "ffbd26b8-560e-44b2-bad6-9ecab7d27ba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialValue(Layer):\n",
    "    \n",
    "    def __init__(self, y0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.y0 = y0\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.y0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474a9bb2-fa76-4340-87d5-3f8edff51e58",
   "metadata": {},
   "source": [
    "# Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "d4ebf547",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z, r):\n",
    "    \n",
    "    ad = y[2] / y[3] + x[0]\n",
    "    dp = tf.maximum(0., 1./k + ad)\n",
    "    dm = tf.maximum(0., 1./k - ad)\n",
    "    \n",
    "    return [\n",
    "        x[1],\n",
    "        -eta * x[0],\n",
    "        lp * tf.exp(-k * dp) - lm * tf.exp(-k * dm),\n",
    "        lp * (x[0] + dp) * tf.exp(-k * dp) - lm * (x[0] - dp) * tf.exp(-k * dm),\n",
    "    ]\n",
    "\n",
    "def s(t, x, y, z, r):\n",
    "    return [[sigma, 0], [0, zeta], [0, 0], [0, 0]]\n",
    "\n",
    "# - dH_dx\n",
    "def f(t, x, y, z, r):\n",
    "    \n",
    "    ad = y[2] / y[3] + x[0]\n",
    "    dp = tf.maximum(0., 1./k + ad)\n",
    "    dm = tf.maximum(0., 1./k - ad)\n",
    "\n",
    "    return [\n",
    "        -(y[3] * lp * tf.exp(-k * dp) - y[3] * lm * tf.exp(-k * dm)),\n",
    "        -(y[0] - eta * y[1]),\n",
    "        -(-2. * phi * x[2]),\n",
    "        -(0.)\n",
    "    ]\n",
    "\n",
    "def v(t, x, y, z, r):\n",
    "    return [[0, 0], [epsilon, -epsilon], [0, 0], [0, 0]]\n",
    "\n",
    "# dg_dx\n",
    "def g(x):\n",
    "    return [x[2], 0., x[0] - 2 * psi * x[2], 1.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "35de6b6c-540e-470c-957e-f435b197d170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dX(i, x, y, z, r, dW, dN):\n",
    "    \n",
    "    t = i * dt\n",
    "    \n",
    "    def drift(arg):\n",
    "        x, y, z, r = arg\n",
    "        return tf.math.multiply(b(t, x, y, z, r), dt)\n",
    "    a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "        \n",
    "    def noise(arg):\n",
    "        x, y, z, r, dW = arg\n",
    "        return tf.tensordot(s(t, x, y, z ,r), dW[i], [[1], [0]])\n",
    "    a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "    def jump(arg):\n",
    "        x, y, z, r, dN = arg\n",
    "        return tf.tensordot(v(t, x, y, z ,r), dN[i], [[1], [0]])\n",
    "    a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "    \n",
    "    return a0 + a1 + a2\n",
    "\n",
    "def dY(i, x, y, z, r, dW, dN):\n",
    "    \n",
    "    t = i * dt\n",
    "\n",
    "    def drift(arg):\n",
    "        x, y, z, r = arg\n",
    "        return tf.math.multiply(f(t, x, y, z, r), dt)\n",
    "    a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "    def noise(arg):\n",
    "        x, y, z, r, dW = arg\n",
    "        return tf.tensordot(z, dW[i], [[1], [0]])\n",
    "    a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "    \n",
    "    def jump(arg):\n",
    "        x, y, z, r, dN = arg\n",
    "        return tf.tensordot(r, dN[i], [[1], [0]])\n",
    "    a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "    \n",
    "    return a0 + a1 + a2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8712e6b3-b6ac-4607-b7a1-1d804d531f69",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ce1f9427-7dbe-479c-8008-a9f239bac9c5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function hx at 0x7f8607e113a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function hx at 0x7f8607e113a0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:AutoGraph could not transform <function hy at 0x7f85fc58bb80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function hy at 0x7f85fc58bb80> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: module 'gast' has no attribute 'Index'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    }
   ],
   "source": [
    "paths = []\n",
    "\n",
    "inputs_dW = Input(shape=(n_timesteps, n_diffusion_factors))\n",
    "inputs_dN = Input(shape=(n_timesteps, n_jump_factors))\n",
    "\n",
    "x0 = tf.Variable([[0., 0., 0., 0.]], trainable=False)\n",
    "y0 = tf.Variable([[5., 5., 5., 5.]], trainable=True)\n",
    "\n",
    "x = InitialValue(x0, name='x_0')(inputs_dW)\n",
    "y = InitialValue(y0, name='y_0')(inputs_dW)\n",
    "\n",
    "z = concatenate([x, y])\n",
    "z = Dense(10, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='z1_0')(z)\n",
    "z = Dense(n_dimensions * n_diffusion_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='z2_0')(z)\n",
    "z = Reshape((n_dimensions, n_diffusion_factors), name='zr_0')(z)\n",
    "\n",
    "r = concatenate([x, y])\n",
    "r = Dense(10, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='r1_0')(r)\n",
    "r = Dense(n_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='r2_0')(r)\n",
    "r = Reshape((n_dimensions, n_jump_factors), name='rr_0')(r)\n",
    "\n",
    "paths += [[x, y, z, r]]\n",
    "\n",
    "@tf.function\n",
    "def hx(i, r):\n",
    "    return r[0] + dX(i, *r)\n",
    "\n",
    "@tf.function\n",
    "def hy(i, r):\n",
    "    return r[1] + dY(i, *r)\n",
    "\n",
    "for i in range(n_timesteps):\n",
    "    \n",
    "    x, y = (\n",
    "        Lambda(lambda r: hx(i, r), name=f'x_{i+1}')([x, y, z, r, inputs_dW, inputs_dN]),\n",
    "        Lambda(lambda r: hy(i, r), name=f'y_{i+1}')([x, y, z, r, inputs_dW, inputs_dN]),\n",
    "    )\n",
    "    \n",
    "    # we don't train z for the last time step; keep for consistency\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(10, activation='relu', name=f'z1_{i+1}')(z)\n",
    "    z = Dense(n_dimensions * n_diffusion_factors, activation='relu', name=f'z2_{i+1}')(z)\n",
    "    z = Reshape((n_dimensions, n_diffusion_factors), name=f'zr_{i+1}')(z)\n",
    "    \n",
    "    # we don't train r for the last time step; keep for consistency\n",
    "    r = concatenate([x, y])\n",
    "    r = Dense(10, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name=f'r1_{i+1}')(r)\n",
    "    r = Dense(n_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name=f'r2_{i+1}')(r)\n",
    "    r = Reshape((n_dimensions, n_jump_factors), name=f'rr_{i+1}')(r)\n",
    "\n",
    "    paths += [[x, y, z, r]]\n",
    "    \n",
    "outputs_loss = Lambda(lambda r: r[1] - tf.transpose(tf.vectorized_map(g, r[0])))([x, y])\n",
    "outputs_paths = tf.stack(\n",
    "    [tf.stack([p[0] for p in paths[1:]], axis=1), tf.stack([p[1] for p in paths[1:]], axis=1)] + \n",
    "    [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_diffusion_factors)] +\n",
    "    [tf.stack([p[3][:, :, i] for p in paths[1:]], axis=1) for i in range(n_jump_factors)], axis=2)\n",
    "\n",
    "model_loss = Model([inputs_dW, inputs_dN], outputs_loss)\n",
    "model_loss.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "# (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "# skips the first time step\n",
    "model_paths = Model([inputs_dW, inputs_dN], outputs_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "3eac4263-05a5-4a19-8f1d-d9c0a8b94c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_45\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_85 (InputLayer)           [(None, 4, 2)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_0 (InitialValue)              (1, 4)               4           input_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "y_0 (InitialValue)              (1, 4)               4           input_85[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_657 (Concatenate)   (1, 8)               0           x_0[0][0]                        \n",
      "                                                                 y_0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_658 (Concatenate)   (1, 8)               0           x_0[0][0]                        \n",
      "                                                                 y_0[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "z1_0 (Dense)                    (1, 10)              90          concatenate_657[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "r1_0 (Dense)                    (1, 10)              90          concatenate_658[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "z2_0 (Dense)                    (1, 8)               88          z1_0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "r2_0 (Dense)                    (1, 8)               88          r1_0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zr_0 (Reshape)                  (1, 4, 2)            0           z2_0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "rr_0 (Reshape)                  (1, 4, 2)            0           r2_0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "input_86 (InputLayer)           [(None, 4, 2)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "x_1 (Lambda)                    (None, 4)            0           x_0[0][0]                        \n",
      "                                                                 y_0[0][0]                        \n",
      "                                                                 zr_0[0][0]                       \n",
      "                                                                 rr_0[0][0]                       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "y_1 (Lambda)                    (None, 4)            0           x_0[0][0]                        \n",
      "                                                                 y_0[0][0]                        \n",
      "                                                                 zr_0[0][0]                       \n",
      "                                                                 rr_0[0][0]                       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_659 (Concatenate)   (None, 8)            0           x_1[0][0]                        \n",
      "                                                                 y_1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_660 (Concatenate)   (None, 8)            0           x_1[0][0]                        \n",
      "                                                                 y_1[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "z1_1 (Dense)                    (None, 10)           90          concatenate_659[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "r1_1 (Dense)                    (None, 10)           90          concatenate_660[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "z2_1 (Dense)                    (None, 8)            88          z1_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "r2_1 (Dense)                    (None, 8)            88          r1_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zr_1 (Reshape)                  (None, 4, 2)         0           z2_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "rr_1 (Reshape)                  (None, 4, 2)         0           r2_1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "x_2 (Lambda)                    (None, 4)            0           x_1[0][0]                        \n",
      "                                                                 y_1[0][0]                        \n",
      "                                                                 zr_1[0][0]                       \n",
      "                                                                 rr_1[0][0]                       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "y_2 (Lambda)                    (None, 4)            0           x_1[0][0]                        \n",
      "                                                                 y_1[0][0]                        \n",
      "                                                                 zr_1[0][0]                       \n",
      "                                                                 rr_1[0][0]                       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_661 (Concatenate)   (None, 8)            0           x_2[0][0]                        \n",
      "                                                                 y_2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_662 (Concatenate)   (None, 8)            0           x_2[0][0]                        \n",
      "                                                                 y_2[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "z1_2 (Dense)                    (None, 10)           90          concatenate_661[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "r1_2 (Dense)                    (None, 10)           90          concatenate_662[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "z2_2 (Dense)                    (None, 8)            88          z1_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "r2_2 (Dense)                    (None, 8)            88          r1_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zr_2 (Reshape)                  (None, 4, 2)         0           z2_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "rr_2 (Reshape)                  (None, 4, 2)         0           r2_2[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "x_3 (Lambda)                    (None, 4)            0           x_2[0][0]                        \n",
      "                                                                 y_2[0][0]                        \n",
      "                                                                 zr_2[0][0]                       \n",
      "                                                                 rr_2[0][0]                       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "y_3 (Lambda)                    (None, 4)            0           x_2[0][0]                        \n",
      "                                                                 y_2[0][0]                        \n",
      "                                                                 zr_2[0][0]                       \n",
      "                                                                 rr_2[0][0]                       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_663 (Concatenate)   (None, 8)            0           x_3[0][0]                        \n",
      "                                                                 y_3[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_664 (Concatenate)   (None, 8)            0           x_3[0][0]                        \n",
      "                                                                 y_3[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "z1_3 (Dense)                    (None, 10)           90          concatenate_663[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "r1_3 (Dense)                    (None, 10)           90          concatenate_664[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "z2_3 (Dense)                    (None, 8)            88          z1_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "r2_3 (Dense)                    (None, 8)            88          r1_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "zr_3 (Reshape)                  (None, 4, 2)         0           z2_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "rr_3 (Reshape)                  (None, 4, 2)         0           r2_3[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "x_4 (Lambda)                    (None, 4)            0           x_3[0][0]                        \n",
      "                                                                 y_3[0][0]                        \n",
      "                                                                 zr_3[0][0]                       \n",
      "                                                                 rr_3[0][0]                       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "y_4 (Lambda)                    (None, 4)            0           x_3[0][0]                        \n",
      "                                                                 y_3[0][0]                        \n",
      "                                                                 zr_3[0][0]                       \n",
      "                                                                 rr_3[0][0]                       \n",
      "                                                                 input_85[0][0]                   \n",
      "                                                                 input_86[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "lambda_28 (Lambda)              (None, 4)            0           x_4[0][0]                        \n",
      "                                                                 y_4[0][0]                        \n",
      "==================================================================================================\n",
      "Total params: 1,432\n",
      "Trainable params: 1,428\n",
      "Non-trainable params: 4\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_loss.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db5cf0e",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "56a90e15-3142-4b85-9946-b92135d1d07c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "dN = tf.random.poisson((n_paths, n_timesteps), [dt * lp, dt * lm])\n",
    "target = tf.zeros((n_paths, n_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "a03915f5-7578-42d4-a664-e90869d936c6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (x_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 4) dtype=float32, numpy=array([[        5,         5,         5,         5]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (y_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 4) dtype=float32, numpy=array([[        5,         5,         5,         5]], dtype=float32)>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f860da0cd30> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=float32, numpy=array([       14,         0,         0,         0,      2031,         0,      2031,         0], dtype=float32)>"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for exploding gradients before training\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = model_loss([dW, dN])\n",
    "\n",
    "# bias of the last dense layer\n",
    "variables = model_loss.variables[-1]\n",
    "tape.gradient(loss, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "603e5ef6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "64/64 [==============================] - 4s 4ms/step - loss: 37.4684\n",
      "Epoch 2/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 29.9652\n",
      "Epoch 3/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 28.0697\n",
      "Epoch 4/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 26.5893\n",
      "Epoch 5/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 24.6950\n",
      "Epoch 6/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 24.1366\n",
      "Epoch 7/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 22.7723\n",
      "Epoch 8/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 22.0850\n",
      "Epoch 9/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 21.1481\n",
      "Epoch 10/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 20.4348\n",
      "Epoch 11/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 19.8322\n",
      "Epoch 12/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 19.0934\n",
      "Epoch 13/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 18.3664\n",
      "Epoch 14/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 17.6931\n",
      "Epoch 15/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 17.2771\n",
      "Epoch 16/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 16.5116\n",
      "Epoch 17/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 16.0251\n",
      "Epoch 18/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 15.4782\n",
      "Epoch 19/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 14.9304\n",
      "Epoch 20/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 14.4979\n",
      "Epoch 21/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 14.1249\n",
      "Epoch 22/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 13.6045\n",
      "Epoch 23/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 13.0853\n",
      "Epoch 24/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 12.7299\n",
      "Epoch 25/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 12.1969\n",
      "Epoch 26/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 11.8421\n",
      "Epoch 27/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 10.7874\n",
      "Epoch 28/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 10.1339\n",
      "Epoch 29/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 11.4862\n",
      "Epoch 30/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 9.9135\n",
      "Epoch 31/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 10.1688\n",
      "Epoch 32/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 10.8250\n",
      "Epoch 33/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 10.7654\n",
      "Epoch 34/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 11.5879\n",
      "Epoch 35/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 9.4208\n",
      "Epoch 36/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 9.3846\n",
      "Epoch 37/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 9.3377\n",
      "Epoch 38/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 9.4455\n",
      "Epoch 39/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 9.0296\n",
      "Epoch 40/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.9859\n",
      "Epoch 41/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.8717\n",
      "Epoch 42/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.7556\n",
      "Epoch 43/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.8115\n",
      "Epoch 44/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.6230\n",
      "Epoch 45/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.6787\n",
      "Epoch 46/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.7989\n",
      "Epoch 47/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.5023\n",
      "Epoch 48/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.2667\n",
      "Epoch 49/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.3753\n",
      "Epoch 50/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.2744\n",
      "Epoch 51/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.2557\n",
      "Epoch 52/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.0845\n",
      "Epoch 53/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.1730\n",
      "Epoch 54/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.1544\n",
      "Epoch 55/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.8266\n",
      "Epoch 56/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.8829\n",
      "Epoch 57/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.9247\n",
      "Epoch 58/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.1223\n",
      "Epoch 59/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.2447\n",
      "Epoch 60/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.9378\n",
      "Epoch 61/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.9947\n",
      "Epoch 62/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 8.0699\n",
      "Epoch 63/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.8391\n",
      "Epoch 64/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.9307\n",
      "Epoch 65/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.8634\n",
      "Epoch 66/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.6161\n",
      "Epoch 67/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.7891\n",
      "Epoch 68/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.8310\n",
      "Epoch 69/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.7696\n",
      "Epoch 70/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.6537\n",
      "Epoch 71/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.6358\n",
      "Epoch 72/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.6919\n",
      "Epoch 73/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.5471\n",
      "Epoch 74/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.5782\n",
      "Epoch 75/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.2993\n",
      "Epoch 76/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.4615\n",
      "Epoch 77/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.3751\n",
      "Epoch 78/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.3620\n",
      "Epoch 79/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.3863\n",
      "Epoch 80/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.3380\n",
      "Epoch 81/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.3101\n",
      "Epoch 82/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.3123\n",
      "Epoch 83/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.2586\n",
      "Epoch 84/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.1081\n",
      "Epoch 85/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.2603\n",
      "Epoch 86/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.1527\n",
      "Epoch 87/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.0776\n",
      "Epoch 88/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.0377\n",
      "Epoch 89/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.0417\n",
      "Epoch 90/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.0149\n",
      "Epoch 91/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.9651\n",
      "Epoch 92/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.0794\n",
      "Epoch 93/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.0244\n",
      "Epoch 94/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 7.0124\n",
      "Epoch 95/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.8820\n",
      "Epoch 96/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.7828\n",
      "Epoch 97/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.8659\n",
      "Epoch 98/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.7965\n",
      "Epoch 99/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.7946\n",
      "Epoch 100/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.9334\n",
      "Epoch 101/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.9243\n",
      "Epoch 102/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.9725\n",
      "Epoch 103/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.8925\n",
      "Epoch 104/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.9564\n",
      "Epoch 105/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.9980\n",
      "Epoch 106/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.8868\n",
      "Epoch 107/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.9447\n",
      "Epoch 108/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.9316\n",
      "Epoch 109/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.6904\n",
      "Epoch 110/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.7829\n",
      "Epoch 111/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.7353\n",
      "Epoch 112/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.7558\n",
      "Epoch 113/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.8010\n",
      "Epoch 114/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.7470\n",
      "Epoch 115/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.8358\n",
      "Epoch 116/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5635\n",
      "Epoch 117/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.7329\n",
      "Epoch 118/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.7236\n",
      "Epoch 119/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.6004\n",
      "Epoch 120/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5689\n",
      "Epoch 121/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.7074\n",
      "Epoch 122/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5243\n",
      "Epoch 123/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5253\n",
      "Epoch 124/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.6446\n",
      "Epoch 125/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5880\n",
      "Epoch 126/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5627\n",
      "Epoch 127/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5654\n",
      "Epoch 128/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5067\n",
      "Epoch 129/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.4027\n",
      "Epoch 130/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5446\n",
      "Epoch 131/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.3853\n",
      "Epoch 132/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5959\n",
      "Epoch 133/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.3332\n",
      "Epoch 134/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.2466\n",
      "Epoch 135/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5479\n",
      "Epoch 136/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5260\n",
      "Epoch 137/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.4038\n",
      "Epoch 138/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.2577\n",
      "Epoch 139/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.5869\n",
      "Epoch 140/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.3276\n",
      "Epoch 141/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.4134\n",
      "Epoch 142/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.4309\n",
      "Epoch 143/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.2246\n",
      "Epoch 144/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.3186\n",
      "Epoch 145/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.2102\n",
      "Epoch 146/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.4618\n",
      "Epoch 147/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.3658\n",
      "Epoch 148/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.3047\n",
      "Epoch 149/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.2271\n",
      "Epoch 150/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.0966\n",
      "Epoch 151/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.2643\n",
      "Epoch 152/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.2075\n",
      "Epoch 153/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.2339\n",
      "Epoch 154/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.0934\n",
      "Epoch 155/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.1382\n",
      "Epoch 156/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.1015\n",
      "Epoch 157/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.1974\n",
      "Epoch 158/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.1343\n",
      "Epoch 159/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.0279\n",
      "Epoch 160/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.0718\n",
      "Epoch 161/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.3750\n",
      "Epoch 162/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.0693\n",
      "Epoch 163/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.1218\n",
      "Epoch 164/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.1482\n",
      "Epoch 165/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.0719\n",
      "Epoch 166/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.1412\n",
      "Epoch 167/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.0846\n",
      "Epoch 168/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.9280\n",
      "Epoch 169/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.0339\n",
      "Epoch 170/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.9318\n",
      "Epoch 171/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.9940\n",
      "Epoch 172/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.0113\n",
      "Epoch 173/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 6.0089\n",
      "Epoch 174/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.9856\n",
      "Epoch 175/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8839\n",
      "Epoch 176/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.9271\n",
      "Epoch 177/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8654\n",
      "Epoch 178/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.9305\n",
      "Epoch 179/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.9430\n",
      "Epoch 180/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.9579\n",
      "Epoch 181/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8831\n",
      "Epoch 182/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.9073\n",
      "Epoch 183/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8763\n",
      "Epoch 184/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7639\n",
      "Epoch 185/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7586\n",
      "Epoch 186/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7577\n",
      "Epoch 187/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8582\n",
      "Epoch 188/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8311\n",
      "Epoch 189/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7267\n",
      "Epoch 190/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8768\n",
      "Epoch 191/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8332\n",
      "Epoch 192/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8631\n",
      "Epoch 193/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8320\n",
      "Epoch 194/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7080\n",
      "Epoch 195/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.9115\n",
      "Epoch 196/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7422\n",
      "Epoch 197/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6276\n",
      "Epoch 198/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8230\n",
      "Epoch 199/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7168\n",
      "Epoch 200/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8529\n",
      "Epoch 201/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6820\n",
      "Epoch 202/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7175\n",
      "Epoch 203/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8797\n",
      "Epoch 204/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7748\n",
      "Epoch 205/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8078\n",
      "Epoch 206/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6156\n",
      "Epoch 207/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6937\n",
      "Epoch 208/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7274\n",
      "Epoch 209/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6567\n",
      "Epoch 210/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7403\n",
      "Epoch 211/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7237\n",
      "Epoch 212/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6847\n",
      "Epoch 213/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.8032\n",
      "Epoch 214/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.5963\n",
      "Epoch 215/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7342\n",
      "Epoch 216/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6620\n",
      "Epoch 217/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7797\n",
      "Epoch 218/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7965\n",
      "Epoch 219/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6375\n",
      "Epoch 220/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.5353\n",
      "Epoch 221/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6017\n",
      "Epoch 222/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6143\n",
      "Epoch 223/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7014\n",
      "Epoch 224/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.4538\n",
      "Epoch 225/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.4920\n",
      "Epoch 226/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6681\n",
      "Epoch 227/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7235\n",
      "Epoch 228/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.5920\n",
      "Epoch 229/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.5381\n",
      "Epoch 230/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.5094\n",
      "Epoch 231/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.7103\n",
      "Epoch 232/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.5595\n",
      "Epoch 233/1000\n",
      "64/64 [==============================] - 0s 4ms/step - loss: 5.6327\n",
      "Epoch 234/1000\n",
      "14/64 [=====>........................] - ETA: 0s - loss: 5.5119"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [274]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m callback \u001b[38;5;241m=\u001b[39m ModelCheckpoint(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_models/weights\u001b[39m\u001b[38;5;132;01m{epoch:04d}\u001b[39;00m\u001b[38;5;124m.h5\u001b[39m\u001b[38;5;124m'\u001b[39m, save_weights_only\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      2\u001b[0m model_loss\u001b[38;5;241m.\u001b[39msave_weights(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_models/weights0000.h5\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m \u001b[43mmodel_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdN\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/tensorflow/python/keras/engine/training.py:1100\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\n\u001b[1;32m   1094\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1095\u001b[0m     epoch_num\u001b[38;5;241m=\u001b[39mepoch,\n\u001b[1;32m   1096\u001b[0m     step_num\u001b[38;5;241m=\u001b[39mstep,\n\u001b[1;32m   1097\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39mbatch_size,\n\u001b[1;32m   1098\u001b[0m     _r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m   1099\u001b[0m   callbacks\u001b[38;5;241m.\u001b[39mon_train_batch_begin(step)\n\u001b[0;32m-> 1100\u001b[0m   tmp_logs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1101\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m data_handler\u001b[38;5;241m.\u001b[39mshould_sync:\n\u001b[1;32m   1102\u001b[0m     context\u001b[38;5;241m.\u001b[39masync_wait()\n",
      "File \u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:828\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trace\u001b[38;5;241m.\u001b[39mTrace(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name) \u001b[38;5;28;01mas\u001b[39;00m tm:\n\u001b[0;32m--> 828\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    829\u001b[0m   compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_experimental_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    830\u001b[0m   new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n",
      "File \u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py:855\u001b[0m, in \u001b[0;36mFunction._call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    852\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    853\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[1;32m    854\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[0;32m--> 855\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stateless_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# pylint: disable=not-callable\u001b[39;00m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stateful_fn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    857\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[1;32m    858\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[1;32m    859\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/tensorflow/python/eager/function.py:2942\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m   2940\u001b[0m   (graph_function,\n\u001b[1;32m   2941\u001b[0m    filtered_flat_args) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2943\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfiltered_flat_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgraph_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/tensorflow/python/eager/function.py:1918\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1914\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[1;32m   1915\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[1;32m   1916\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[1;32m   1917\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[0;32m-> 1918\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_call_outputs(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1919\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcancellation_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcancellation_manager\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1920\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[1;32m   1921\u001b[0m     args,\n\u001b[1;32m   1922\u001b[0m     possible_gradient_type,\n\u001b[1;32m   1923\u001b[0m     executing_eagerly)\n\u001b[1;32m   1924\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/tensorflow/python/eager/function.py:555\u001b[0m, in \u001b[0;36m_EagerDefinedFunction.call\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _InterpolateFunctionError(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    554\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m cancellation_manager \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 555\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msignature\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_num_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    558\u001b[0m \u001b[43m        \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    559\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    560\u001b[0m \u001b[43m        \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    561\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    562\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[1;32m    563\u001b[0m         \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mname),\n\u001b[1;32m    564\u001b[0m         num_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_outputs,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    567\u001b[0m         ctx\u001b[38;5;241m=\u001b[39mctx,\n\u001b[1;32m    568\u001b[0m         cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_manager)\n",
      "File \u001b[0;32m~/.conda/envs/default/lib/python3.9/site-packages/tensorflow/python/eager/execute.py:59\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     58\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 59\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     62\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "callback = ModelCheckpoint('_models/weights{epoch:04d}.h5', save_weights_only=True, overwrite=True)\n",
    "model_loss.save_weights('_models/weights0000.h5')\n",
    "model_loss.fit([dW, dN], target, batch_size=batch_size, epochs=1000, callbacks=[callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "f30a1e40-4b94-467d-9594-033ab2612be3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(1, 4) dtype=float32, numpy=array([[    3.387,     3.427,     3.703,     3.433]], dtype=float32)>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_loss.variables[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e844d499",
   "metadata": {},
   "source": [
    "# Display paths and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "6f13900f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bad model\n",
    "model_loss.load_weights('_models/weights0001.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "71d789ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:11 out of the last 11 calls to <function pfor.<locals>.f at 0x7f86007ddb80> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[-2.7892856e+05,  5.7580199e+04,  2.9229816e+04,  2.2272785e+04],\n",
       "       [-3.0421126e+11,  5.4906335e+02,  2.0158292e+11, -2.4858957e+01],\n",
       "       [-1.4093776e+02,  7.8264332e+00,  1.3364545e+02, -2.5493622e-01],\n",
       "       [-2.7537908e+03,  6.6549384e+02,  3.6594989e+02,  2.9560254e+02],\n",
       "       [-3.4703342e+01,  9.8967619e+00,  3.0825493e+01,  2.0972550e+00],\n",
       "       [-3.3865441e+05,  7.6042847e+01,  1.9492478e+05, -5.9934139e-01],\n",
       "       [-1.1157483e+01,  4.2878218e+00,  2.1506187e+01, -4.2990202e-01],\n",
       "       [-2.3254272e+03,  1.1686725e+02,  7.2250928e+02,  2.4598133e+01],\n",
       "       [-2.4142380e+01,  6.7904444e+00,  3.3732925e+01, -1.9588035e-01],\n",
       "       [-8.0501183e+01,  1.8984909e+01,  5.0932274e+01,  5.5728073e+00],\n",
       "       [-5.8120212e+05,  1.1134502e+01,  6.7598969e+05, -2.3334920e+00],\n",
       "       [-3.7009297e+04,  1.0674642e+04,  2.2117695e+03,  4.3615493e+03],\n",
       "       [-7.7775528e+01,  1.8671093e+01,  4.9698235e+01,  5.4542475e+00],\n",
       "       [-1.4375246e+01,  5.8834391e+00,  2.1985283e+01, -9.2938840e-02],\n",
       "       [-2.8718988e+07,  4.8269255e+06,  2.1832348e+06,  1.7811381e+06],\n",
       "       [-5.6691078e+01,  1.0624148e+01,  4.6834194e+01,  1.5728052e+00]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model_loss([dW, dN]).numpy()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "99074637",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = model_paths([dW, dN]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "e5b0f8b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[    1.548,     2.081,     2.594,     3.082,     3.539,      3.96,     4.341,     4.678,     4.966,     5.203],\n",
       "        [   0.8524,    0.6499,    0.3942,   0.08711,   -0.2687,   -0.6702,    -1.114,    -1.596,    -2.111,    -2.655],\n",
       "        [    1.267,     1.784,     2.698,     4.522,     10.15,     26.01,     117.8,     128.1, 1.113e+04,  1.34e+04],\n",
       "        [     1.01,    0.9629,    0.8447,    0.2821,    -5.056,    -29.29,      -292,    -295.1, -7.657e+04, -8.803e+04]],\n",
       "\n",
       "       [[    1.466,    0.3521,    -1.664,    -5.541,    -17.31,    -48.15,    -271.1,    -439.2, -2.043e+05, -2.655e+05],\n",
       "        [        2,     2.252,      2.99,     3.405,       4.3,     6.539,     20.14,     61.24,     132.6, 5.758e+04],\n",
       "        [    2.197,      2.45,     2.807,     4.071,     5.281,     9.444,     24.07,     146.9,     212.5,      2439],\n",
       "        [    1.996,     2.164,     2.144,     2.089,     2.089,     2.445,     18.52,     18.52,     35.06, 2.227e+04]],\n",
       "\n",
       "       [[        0,         0,    0.2226,         0,     5.527,      3.27,     53.02,         0, 4.213e+04,  1.97e+04],\n",
       "        [   0.4439,     1.222,         0,         0,     0.174,     18.16,     26.72,     64.33,  9.42e+04, 4.461e+04],\n",
       "        [        0,         0,     1.615,    0.6825,     5.293,      21.4,     221.6,     89.16,         0,         0],\n",
       "        [   0.3749,         0,         0,         0,     1.041,     37.08,         0,     47.56, 5.557e+04, 3.162e+04]],\n",
       "\n",
       "       [[    1.707,    0.8139,     1.409,    0.2595,     3.831,         0,         0,         0, 1.489e+04, 4.217e+04],\n",
       "        [        0,         0,     1.055,         0,         0,         0,         0,     156.8, 1.093e+05, 2.528e+04],\n",
       "        [        0,         0,         0,         0,     5.001,     3.578,         0,         0,         0, 1.751e+04],\n",
       "        [        0,    0.4098,     1.152,         0,     2.317,     11.27,         0,     100.4, 5.586e+04,         0]],\n",
       "\n",
       "       [[        0,         0,    0.1001,         0,         0,         0,         0,    0.9458,         0,       125],\n",
       "        [        0,     0.302,    0.1044,    0.1484,         0,         0,         0,      21.7, 1.073e+04,         0],\n",
       "        [        0,    0.2335,         0,         0,         0,         0,         0,     8.591,         0,      1438],\n",
       "        [  0.05251,    0.0917,    0.1804,    0.2495,         0,    0.7751,     8.188,         0,      4585,         0]],\n",
       "\n",
       "       [[ 0.003962,         0,   0.02763,         0,         0,         0,     5.435,     6.515, 1.485e+04,         0],\n",
       "        [        0,         0,   0.06342,         0,         0,     2.816,     1.305,         0,      2250,         0],\n",
       "        [   0.1256,         0,    0.1362,   0.08126,    0.2855,         0,         0,         0,      2716,         0],\n",
       "        [        0,   0.04739,    0.1666,    0.3794,         0,         0,     8.695,     22.33,         0,      4090]]], dtype=float32)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%9.4g\" % x))\n",
    "tf.transpose(paths[0, :, :, :], (1, 2, 0)).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
