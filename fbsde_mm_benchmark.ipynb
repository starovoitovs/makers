{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e8dd08",
   "metadata": {},
   "source": [
    "# FBSDE\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d413ce-59dc-42d7-9d78-dce4a53044d6",
   "metadata": {},
   "source": [
    "Consider an FBSDE:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t^i &= \\exp(-X_t^i) \\sum_j Z^{ij}_t dW^i_t + \\exp(-X_t^i) \\sum_j r^{ij}_t dN^i_t & X_0 = x_0 \\\\\n",
    "dY_t^i &=  \\frac 12 \\exp(-X^i_t) \\sum_j (Z^{ij}_t)^2 dt + \\sum_j Z^{ij}_t dW^i_t + \\sum_j r^{ij}_t dN^i_t & Y_T= \\exp(X^i_T)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "One can see by Ito's lemma that\n",
    "\n",
    "$$\n",
    "Y^i_t = \\exp(X^i_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37f9f24-e9e1-4a9a-95e8-3fb2b0396ffc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# working configuration: time_horizon = 0.1, n_paths = 2 ** 12, n_timesteps = 4, n_dimensions = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26fff9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from makers.gpu_utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(pick_gpu_lowest_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ecf9b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate, Layer, BatchNormalization\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.metrics import mse\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from itertools import product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b8cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967ebe5",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "6c0b3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical parameters\n",
    "n_paths = 2 ** 16\n",
    "n_timesteps = 4\n",
    "\n",
    "# model parameters\n",
    "time_horizon = 0.1\n",
    "n_dimensions = 10\n",
    "n_diffusion_factors = 10\n",
    "n_jump_factors = 10\n",
    "dt = time_horizon / n_timesteps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d9b36d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z, r):\n",
    "    return [0. for _ in range(n_dimensions)]\n",
    "\n",
    "def s(t, x, y, z, r):\n",
    "    return [[tf.exp(-x[i]) * z[i][j] for j in range(n_diffusion_factors)] for i in range(n_dimensions)]\n",
    "\n",
    "def f(t, x, y, z, r):\n",
    "    return [tf.exp(-x[i]) * tf.reduce_sum(z[i] ** 2) / 2. for i in range(n_dimensions)]\n",
    "\n",
    "def v(t, x, y, z, r):\n",
    "    return [[0. for _ in range(n_jump_factors)] for _ in range(n_dimensions)]\n",
    "\n",
    "def g(x):\n",
    "    return [tf.exp(x[i]) for i in range(n_dimensions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120171a2",
   "metadata": {},
   "source": [
    "# Custom layers and callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "750f491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialValue(Layer):\n",
    "    \n",
    "    def __init__(self, y0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.y0 = y0\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.y0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c6850f6-d556-474b-abaf-0239a7ee2af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Y0Callback(Callback):\n",
    "    \n",
    "    def __init__(self, filepath=None):\n",
    "        super(Y0Callback, self).__init__()\n",
    "        self.filepath = filepath\n",
    "        self.y0s = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y0 = model_loss.variables[1].numpy()[0]\n",
    "        self.y0s += [y0]\n",
    "        print(f\"{y0}\\n\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        if self.filepath is not None:\n",
    "            pd.DataFrame(self.y0s).to_csv(self.filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cece56b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "a7ab7b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def build_model(n_dimensions, n_diffusion_factors, n_jump_factors, n_timesteps, time_horizon):\n",
    "    \n",
    "    dt = time_horizon / n_timesteps\n",
    "    \n",
    "    def dX(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(b(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.tensordot(s(t, x, y, z, r), dW, [[1], [0]])\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.tensordot(v(t, x, y, z, r), dN, [[1], [0]])\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    def dY(t, x, y, z, r, dW, dN):\n",
    "\n",
    "        def drift(arg):\n",
    "            x, y, z, r = arg\n",
    "            return tf.math.multiply(f(t, x, y, z, r), dt)\n",
    "        a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "        def noise(arg):\n",
    "            x, y, z, r, dW = arg\n",
    "            return tf.tensordot(z, dW, [[1], [0]])\n",
    "        a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "        def jump(arg):\n",
    "            x, y, z, r, dN = arg\n",
    "            return tf.tensordot(r, dN, [[1], [0]])\n",
    "        a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "\n",
    "        return a0 + a1 + a2\n",
    "\n",
    "    @tf.function\n",
    "    def hx(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return x + dX(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "    @tf.function\n",
    "    def hy(args):\n",
    "        i, x, y, z, r, dW, dN = args\n",
    "        return y + dY(i * dt, x, y, z, r, dW, dN)\n",
    "    \n",
    "    paths = []\n",
    "\n",
    "    n_hidden_units = n_dimensions + n_diffusion_factors + n_jump_factors + 10\n",
    "\n",
    "    inputs_dW = Input(shape=(n_timesteps, n_diffusion_factors))\n",
    "    inputs_dN = Input(shape=(n_timesteps, n_jump_factors))\n",
    "\n",
    "    x0 = tf.Variable([[1. for _ in range(n_dimensions)]], trainable=False)\n",
    "    y0 = tf.Variable([[0. for _ in range(n_dimensions)]], trainable=True)\n",
    "\n",
    "    x = InitialValue(x0, trainable=False, name='x_0')(inputs_dW)\n",
    "    y = InitialValue(y0, trainable=True, name='y_0')(inputs_dW)\n",
    "\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='z1_0')(z)\n",
    "    z = Dense(n_dimensions * n_diffusion_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='z2_0')(z)\n",
    "    z = BatchNormalization(name='zbn_0')(z)\n",
    "    z = Reshape((n_dimensions, n_diffusion_factors), name='zr_0')(z)\n",
    "\n",
    "    r = concatenate([x, y])\n",
    "    r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='r1_0')(r)\n",
    "    r = Dense(n_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='r2_0')(r)\n",
    "    r = BatchNormalization(name='rbn_0')(r)\n",
    "    r = Reshape((n_dimensions, n_jump_factors), name='rr_0')(r)\n",
    "\n",
    "    paths += [[x, y, z, r]]\n",
    "\n",
    "    # pre-compile lambda layers\n",
    "\n",
    "    for i in range(n_timesteps):\n",
    "\n",
    "        step = InitialValue(tf.Variable(i, dtype=tf.float32, trainable=False))(inputs_dW)\n",
    "\n",
    "        dW = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dW, step])\n",
    "        dN = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dN, step])\n",
    "\n",
    "        x, y = (\n",
    "            Lambda(hx, name=f'x_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "            Lambda(hy, name=f'y_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "        )\n",
    "\n",
    "        # we don't train z for the last time step; keep for consistency\n",
    "        z = concatenate([x, y])\n",
    "        z = Dense(n_hidden_units, activation='relu', name=f'z1_{i+1}')(z)\n",
    "        z = Dense(n_dimensions * n_diffusion_factors, activation='relu', name=f'z2_{i+1}')(z)\n",
    "        z = Reshape((n_dimensions, n_diffusion_factors), name=f'zr_{i+1}')(z)\n",
    "        z = BatchNormalization(name=f'zbn_{i+1}')(z)\n",
    "\n",
    "        # we don't train r for the last time step; keep for consistency\n",
    "        r = concatenate([x, y])\n",
    "        r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name=f'r1_{i+1}')(r)\n",
    "        r = Dense(n_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name=f'r2_{i+1}')(r)\n",
    "        r = Reshape((n_dimensions, n_jump_factors), name=f'rr_{i+1}')(r)\n",
    "        r = BatchNormalization(name=f'rbn_{i+1}')(r)\n",
    "\n",
    "        paths += [[x, y, z, r]]\n",
    "\n",
    "    outputs_loss = Lambda(lambda r: r[1] - tf.transpose(tf.vectorized_map(g, r[0])))([x, y])\n",
    "    outputs_paths = tf.stack(\n",
    "        [tf.stack([p[0] for p in paths[1:]], axis=1), tf.stack([p[1] for p in paths[1:]], axis=1)] + \n",
    "        [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_diffusion_factors)] +\n",
    "        [tf.stack([p[3][:, :, i] for p in paths[1:]], axis=1) for i in range(n_jump_factors)], axis=2)\n",
    "\n",
    "\n",
    "    model_loss = Model([inputs_dW, inputs_dN], outputs_loss)\n",
    "\n",
    "    # (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "    # skips the first time step\n",
    "    model_paths = Model([inputs_dW, inputs_dN], outputs_paths)\n",
    "\n",
    "    return model_loss, model_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb207330",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71893d",
   "metadata": {},
   "source": [
    "# Transfer weights (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try transfer learning from another starting point\n",
    "\n",
    "model_loss.get_layer('y_0').set_weights(m_large.get_layer('y_0').get_weights())\n",
    "\n",
    "for i in range(n_timesteps):\n",
    "    model_loss.get_layer(f'z1_{i}').set_weights(m_large.get_layer(f'z1_{i}').get_weights())\n",
    "    model_loss.get_layer(f'z2_{i}').set_weights(m_large.get_layer(f'z2_{i}').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747f722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transfer learning from cruder discretization\n",
    "\n",
    "model_loss.get_layer('y_0').set_weights(m_small.get_layer('y_0').get_weights())\n",
    "\n",
    "n_small = 4\n",
    "\n",
    "for i in range(n_small):\n",
    "    for j in range(n_timesteps // n_small):\n",
    "        model_loss.get_layer(f'z1_{n_timesteps // n_small * i}').set_weights(m_small.get_layer(f'z1_{i}').get_weights())\n",
    "        model_loss.get_layer(f'z2_{n_timesteps // n_small * i}').set_weights(m_small.get_layer(f'z2_{i}').get_weights())\n",
    "        model_loss.get_layer(f'z1_{n_timesteps // n_small * i + j}').set_weights(m_small.get_layer(f'z1_{i}').get_weights())\n",
    "        model_loss.get_layer(f'z2_{n_timesteps // n_small * i + j}').set_weights(m_small.get_layer(f'z2_{i}').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34961895-e7be-41c6-bcb3-305c670f42e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for exploding gradients before training\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = mse(model_loss([dW, dN]), target)\n",
    "\n",
    "# bias of the last dense layer\n",
    "variables = model_loss.variables[-1]\n",
    "tape.gradient(loss, variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093dfa87-0ff7-47c8-8d0c-9c3a95e34551",
   "metadata": {},
   "source": [
    "# Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "b05a3918-af9b-48e0-8796-bfa472b869aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# different termination patience for different n_paths\n",
    "patience = {\n",
    "    2**4: 50,\n",
    "    2**8: 50,\n",
    "    2**12: 3,\n",
    "    2**16: 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85bc1ae9-42c8-44e4-a6ef-f774655eb159",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/24 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (x_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n",
      "WARNING:tensorflow:\n",
      "The following Variables were used a Lambda layer's call (y_1), but\n",
      "are not present in its tracked objects:\n",
      "  <tf.Variable 'Variable:0' shape=(1, 2) dtype=float32>\n",
      "It is possible that this is intended behavior, but it is more likely\n",
      "an omission. This is a strong indication that this layer should be\n",
      "formulated as a subclassed Layer rather than a Lambda layer.\n"
     ]
    }
   ],
   "source": [
    "for time_horizon, n_paths, n_timesteps, n_dimensions in tqdm(list(product([0.1, 1.], [2**8, 2**16], [4, 16], [2, 50]))):\n",
    "    \n",
    "    # create model\n",
    "    dt = time_horizon / n_timesteps\n",
    "    model_loss, model_paths = build_model(n_dimensions=n_dimensions, n_diffusion_factors=n_diffusion_factors, n_jump_factors=n_jump_factors, n_timesteps=n_timesteps, time_horizon=time_horizon)\n",
    "    \n",
    "    # generate paths\n",
    "    dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "    dN = tf.random.poisson((n_paths, n_timesteps), tf.constant(dt * np.array([1. for _ in range(n_jump_factors)]).transpose().reshape(-1)))\n",
    "    target = tf.zeros((n_paths, n_dimensions))\n",
    "    \n",
    "    # create dirs\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    model_name = f\"{timestamp}__{n_dimensions}__{n_paths}__{n_timesteps}__{time_horizon}\"\n",
    "    tb_log_dir = \"/home/tmp/starokon/tensorboard/\" + model_name\n",
    "    output_dir = f\"_output/models/{model_name}\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # define callbacks\n",
    "    y0_callback = Y0Callback(filepath=os.path.join(output_dir, \"y0.csv\"))\n",
    "    checkpoint_callback = ModelCheckpoint(os.path.join(output_dir, \"model.h5\"), monitor=\"loss\", save_weights_only=True, save_best_only=True, overwrite=True)\n",
    "    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_log_dir, histogram_freq=1, profile_batch=1)\n",
    "    nan_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "    loss_callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=1e-4, patience=patience[n_paths])\n",
    "    \n",
    "    # callbacks\n",
    "    callbacks = [\n",
    "        nan_callback,\n",
    "        checkpoint_callback,\n",
    "        # tensorboard_callback,\n",
    "        y0_callback,\n",
    "        loss_callback,\n",
    "    ]\n",
    "    \n",
    "    # leave it here to readjust learning rate on the fly\n",
    "    adam = Adam(learning_rate=1e-3) \n",
    "    model_loss.compile(loss='mse', optimizer=adam)\n",
    "    \n",
    "    # train\n",
    "    history = model_loss.fit([dW, dN], target, batch_size=128, initial_epoch=0, epochs=3000, callbacks=callbacks)\n",
    "    df_loss = pd.DataFrame(history.history['loss'])\n",
    "    df_loss.to_csv(os.path.join(output_dir, 'loss.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c335854-e926-4ca6-adf2-431f24826100",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "41902956",
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "dN = tf.random.poisson((n_paths, n_timesteps), tf.constant(dt * np.array([1. for _ in range(n_jump_factors)]).transpose().reshape(-1)))\n",
    "target = tf.zeros((n_paths, n_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d03606-08d4-41e7-beb5-d5a6f3e002cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard log dir\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"{timestamp}__{n_dimensions}__{n_paths}__{n_timesteps}__{time_horizon}\"\n",
    "tb_log_dir = \"/home/tmp/starokon/tensorboard/\" + model_name\n",
    "output_dir = f\"_output/models/{model_name}\"\n",
    "os.makedirs(output_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6692cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=11, linewidth=90, formatter=dict(float=lambda x: \"%7.5g\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "5d9c71ab-7290-4615-843f-88735a45f5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks\n",
    "y0_callback = Y0Callback(filepath=os.path.join(output_dir, \"y0.csv\"))\n",
    "checkpoint_callback = ModelCheckpoint(os.path.join(output_dir, \"model.h5\"), monitor=\"loss\", save_weights_only=True, save_best_only=True, overwrite=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=tb_log_dir, histogram_freq=1, profile_batch=1)\n",
    "nan_callback = tf.keras.callbacks.TerminateOnNaN()\n",
    "loss_callback = tf.keras.callbacks.EarlyStopping(monitor=\"loss\", min_delta=1e-4, patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "7b6de868-adb5-47db-afae-81af15bb1a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# leave it here to readjust learning rate on the fly\n",
    "adam = Adam(learning_rate=1e-3) \n",
    "model_loss.compile(loss='mse', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "2dbf6f76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model failed to serialize as JSON. Ignoring... Layer InitialValue has arguments in `__init__` and therefore must override `get_config`.\n",
      "Epoch 1/2\n",
      "  6/512 [..............................] - ETA: 10:43 - loss: 6.4761e-07WARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.0400s vs `on_train_batch_end` time: 1.0262s). Check your callbacks.\n",
      "512/512 [==============================] - 42s 46ms/step - loss: 1.2396e-07\n",
      "[ 2.7182  2.7182  2.7182  2.7182  2.7182  2.7182  2.7182  2.7182  2.7182  2.7182]\n",
      "\n",
      "Epoch 2/2\n",
      "512/512 [==============================] - 17s 34ms/step - loss: 9.7734e-08\n",
      "[ 2.7184  2.7183  2.7184  2.7184  2.7184  2.7184  2.7183  2.7183  2.7183  2.7183]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "history = model_loss.fit([dW, dN], target, batch_size=128, initial_epoch=0, epochs=2, callbacks=[nan_callback, checkpoint_callback, tensorboard_callback, y0_callback, loss_callback])\n",
    "df_loss = pd.DataFrame(history.history['loss'])\n",
    "df_loss.to_csv(os.path.join(output_dir, 'loss.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d203e6-7d4f-4cb6-bd0f-23ad8361c0b5",
   "metadata": {},
   "source": [
    "# Validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e3f366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 4s 2ms/step - loss: 3.4178e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.417778726699794e-09"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dW_test = tf.sqrt(dt) * tf.random.normal((n_paths//8, n_timesteps, n_diffusion_factors))\n",
    "dN_test = tf.random.poisson((n_paths//8, n_timesteps), tf.constant(dt * np.array([1., 1.]).transpose().reshape(-1)))\n",
    "target_test = tf.zeros((n_paths//8, n_dimensions))\n",
    "\n",
    "model_loss.evaluate([dW_test, dN_test], target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba9886",
   "metadata": {},
   "source": [
    "# Display paths and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bad model\n",
    "model_loss.load_weights('_models/weights0011.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e765bc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.21302719e-03, -1.83777374e-04,  7.68974721e-02, ...,\n",
       "        -1.08535380e-04, -3.65010686e-02, -1.43885612e-04],\n",
       "       [ 3.12728644e-03, -1.86214020e-04,  1.65139437e-02, ...,\n",
       "        -1.07569365e-04,  9.47822630e-02, -1.43885612e-04],\n",
       "       [-1.13846944e-03, -1.84155477e-04,  3.47495675e-02, ...,\n",
       "        -1.07487824e-04, -5.74785471e-03, -1.43885612e-04],\n",
       "       ...,\n",
       "       [-4.19508014e-03, -1.82968608e-04,  9.62089002e-03, ...,\n",
       "        -1.07971777e-04, -5.63402846e-03, -1.43885612e-04],\n",
       "       [-1.20397564e-02, -1.82203512e-04,  1.12469167e-01, ...,\n",
       "        -1.08954977e-04, -2.62902267e-02, -1.43885612e-04],\n",
       "       [-3.71439802e-03, -1.83611395e-04,  4.11325842e-02, ...,\n",
       "        -1.07773740e-04,  1.11734495e-02, -1.43885612e-04]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model_loss([dW, dN]).numpy()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28f074bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = model_paths([dW, dN]).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
