{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0e8dd08",
   "metadata": {},
   "source": [
    "# FBSDE\n",
    "\n",
    "Ji, Shaolin, Shige Peng, Ying Peng, and Xichuan Zhang. “Three Algorithms for Solving High-Dimensional Fully-Coupled FBSDEs through Deep Learning.” ArXiv:1907.05327 [Cs, Math], February 2, 2020. http://arxiv.org/abs/1907.05327."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d413ce-59dc-42d7-9d78-dce4a53044d6",
   "metadata": {},
   "source": [
    "Consider an FBSDE:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "dX_t^i &= \\exp(-X_t^i) \\sum_j Z^{ij}_t dW^i_t & X_0 = 1\\\\\n",
    "dY_t^i &= \\sum_j Z^{ij}_t dW^i_t + \\frac 12 \\exp(-X^i_t) \\sum_j (Z^{ij}_t)^2 dt & Y_T = \\exp(X^i_T)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "One can see by Ito's lemma that\n",
    "\n",
    "$$\n",
    "Y^i_t = \\exp(X^i_t)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26fff9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from makers.gpu_utils import *\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(pick_gpu_lowest_memory())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecf9b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input, Dense, Lambda, Reshape, concatenate, Layer, BatchNormalization\n",
    "from keras import Model, initializers\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "from keras.metrics import mse\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4b8cbc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices(\"GPU\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5967ebe5",
   "metadata": {},
   "source": [
    "# Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "6c0b3e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical parameters\n",
    "n_paths = 2 ** 16\n",
    "n_timesteps = 16\n",
    "T = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "d9b36d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def b(t, x, y, z, r):\n",
    "    return [0., 0.]\n",
    "\n",
    "def s(t, x, y, z, r):\n",
    "    return [\n",
    "        [tf.exp(-x[0]) * z[0][0], tf.exp(-x[0]) * z[0][1]],\n",
    "        [tf.exp(-x[1]) * z[1][0], tf.exp(-x[1]) * z[1][1]]\n",
    "    ]\n",
    "\n",
    "def f(t, x, y, z, r):\n",
    "    return [\n",
    "        tf.exp(-x[0]) * (z[0][0]**2 + z[0][1]**2) / 2.,\n",
    "        tf.exp(-x[1]) * (z[1][0]**2 + z[1][1]**2) / 2.,\n",
    "    ]\n",
    "\n",
    "def v(t, x, y, z, r):\n",
    "    return [[0., 0.], [0., 0.]]\n",
    "\n",
    "def g(x):\n",
    "    return [tf.exp(x[0]), tf.exp(x[1])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e3aba7",
   "metadata": {},
   "source": [
    "# Automatic parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "286153c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_dimensions = 2\n",
    "n_diffusion_factors = 2\n",
    "n_jump_factors = 2\n",
    "dt = T / n_timesteps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120171a2",
   "metadata": {},
   "source": [
    "# Initial value layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "750f491d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InitialValue(Layer):\n",
    "    \n",
    "    def __init__(self, y0, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.y0 = y0\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return self.y0\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cece56b",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9c4fd921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dX(t, x, y, z, r, dW, dN):\n",
    "    \n",
    "    def drift(arg):\n",
    "        x, y, z, r = arg\n",
    "        return tf.math.multiply(b(t, x, y, z, r), dt)\n",
    "    a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "        \n",
    "    def noise(arg):\n",
    "        x, y, z, r, dW = arg\n",
    "        return tf.tensordot(s(t, x, y, z, r), dW, [[1], [0]])\n",
    "    a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "\n",
    "    def jump(arg):\n",
    "        x, y, z, r, dN = arg\n",
    "        return tf.tensordot(v(t, x, y, z, r), dN, [[1], [0]])\n",
    "    a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "        \n",
    "    return a0 + a1 + a2\n",
    "\n",
    "def dY(t, x, y, z, r, dW, dN):\n",
    "    \n",
    "    def drift(arg):\n",
    "        x, y, z, r = arg\n",
    "        return tf.math.multiply(f(t, x, y, z, r), dt)\n",
    "    a0 = tf.vectorized_map(drift, (x, y, z, r))\n",
    "\n",
    "    def noise(arg):\n",
    "        x, y, z, r, dW = arg\n",
    "        return tf.tensordot(z, dW, [[1], [0]])\n",
    "    a1 = tf.vectorized_map(noise, (x, y, z, r, dW))\n",
    "    \n",
    "    def jump(arg):\n",
    "        x, y, z, r, dN = arg\n",
    "        return tf.tensordot(r, dN, [[1], [0]])\n",
    "    a2 = tf.vectorized_map(jump, (x, y, z, r, dN))\n",
    "    \n",
    "    return a0 + a1 + a2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a7ab7b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "paths = []\n",
    "\n",
    "n_hidden_units = n_dimensions + n_diffusion_factors + n_jump_factors + 10\n",
    "\n",
    "inputs_dW = Input(shape=(n_timesteps, n_diffusion_factors))\n",
    "inputs_dN = Input(shape=(n_timesteps, n_jump_factors))\n",
    "\n",
    "x0 = tf.Variable([[1., 2.]], trainable=False)\n",
    "y0 = tf.Variable([[0., 0.]], trainable=True)\n",
    "\n",
    "x = InitialValue(x0, trainable=False, name='x_0')(inputs_dW)\n",
    "y = InitialValue(y0, trainable=True, name='y_0')(inputs_dW)\n",
    "\n",
    "z = concatenate([x, y])\n",
    "z = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='z1_0')(z)\n",
    "z = Dense(n_dimensions * n_diffusion_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='z2_0')(z)\n",
    "z = BatchNormalization(name='zbn_0')(z)\n",
    "z = Reshape((n_dimensions, n_diffusion_factors), name='zr_0')(z)\n",
    "\n",
    "r = concatenate([x, y])\n",
    "r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='r1_0')(r)\n",
    "r = Dense(n_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name='r2_0')(r)\n",
    "r = BatchNormalization(name='rbn_0')(r)\n",
    "r = Reshape((n_dimensions, n_jump_factors), name='rr_0')(r)\n",
    "\n",
    "paths += [[x, y, z, r]]\n",
    "\n",
    "# pre-compile lambda layers\n",
    "\n",
    "@tf.function\n",
    "def hx(args):\n",
    "    i, x, y, z, r, dW, dN = args\n",
    "    return x + dX(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "@tf.function\n",
    "def hy(args):\n",
    "    i, x, y, z, r, dW, dN = args\n",
    "    return y + dY(i * dt, x, y, z, r, dW, dN)\n",
    "\n",
    "for i in range(n_timesteps):\n",
    "    \n",
    "    step = InitialValue(tf.Variable(i, dtype=tf.float32, trainable=False))(inputs_dW)\n",
    "    \n",
    "    dW = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dW, step])\n",
    "    dN = Lambda(lambda x: x[0][:, tf.cast(x[1], tf.int32)])([inputs_dN, step])\n",
    "    \n",
    "    x, y = (\n",
    "        Lambda(hx, name=f'x_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "        Lambda(hy, name=f'y_{i+1}')([step, x, y, z, r, dW, dN]),\n",
    "    )\n",
    "        \n",
    "    # we don't train z for the last time step; keep for consistency\n",
    "    z = concatenate([x, y])\n",
    "    z = Dense(n_hidden_units, activation='relu', name=f'z1_{i+1}')(z)\n",
    "    z = Dense(n_dimensions * n_diffusion_factors, activation='relu', name=f'z2_{i+1}')(z)\n",
    "    z = Reshape((n_dimensions, n_diffusion_factors), name=f'zr_{i+1}')(z)\n",
    "    z = BatchNormalization(name=f'zbn_{i+1}')(z)\n",
    "    \n",
    "    # we don't train r for the last time step; keep for consistency\n",
    "    r = concatenate([x, y])\n",
    "    r = Dense(n_hidden_units, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name=f'r1_{i+1}')(r)\n",
    "    r = Dense(n_dimensions * n_jump_factors, activation='relu', kernel_initializer=initializers.RandomNormal(stddev=1e-1), name=f'r2_{i+1}')(r)\n",
    "    r = Reshape((n_dimensions, n_jump_factors), name=f'rr_{i+1}')(r)\n",
    "    r = BatchNormalization(name=f'rbn_{i+1}')(r)\n",
    "\n",
    "    paths += [[x, y, z, r]]\n",
    "    \n",
    "outputs_loss = Lambda(lambda r: r[1] - tf.transpose(tf.vectorized_map(g, r[0])))([x, y])\n",
    "outputs_paths = tf.stack(\n",
    "    [tf.stack([p[0] for p in paths[1:]], axis=1), tf.stack([p[1] for p in paths[1:]], axis=1)] + \n",
    "    [tf.stack([p[2][:, :, i] for p in paths[1:]], axis=1) for i in range(n_diffusion_factors)] +\n",
    "    [tf.stack([p[3][:, :, i] for p in paths[1:]], axis=1) for i in range(n_jump_factors)], axis=2)\n",
    "\n",
    "\n",
    "model_loss = Model([inputs_dW, inputs_dN], outputs_loss)\n",
    "\n",
    "# (n_sample, n_timestep, x/y/z_k, n_dimension)\n",
    "# skips the first time step\n",
    "model_paths = Model([inputs_dW, inputs_dN], outputs_paths)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb207330",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_loss.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c71893d",
   "metadata": {},
   "source": [
    "# Transfer weights (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a900d637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try transfer learning from another starting point\n",
    "\n",
    "model_loss.get_layer('y_0').set_weights(m_large.get_layer('y_0').get_weights())\n",
    "\n",
    "for i in range(n_timesteps):\n",
    "    model_loss.get_layer(f'z1_{i}').set_weights(m_large.get_layer(f'z1_{i}').get_weights())\n",
    "    model_loss.get_layer(f'z2_{i}').set_weights(m_large.get_layer(f'z2_{i}').get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5747f722",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# transfer learning from cruder discretization\n",
    "\n",
    "model_loss.get_layer('y_0').set_weights(m_small.get_layer('y_0').get_weights())\n",
    "\n",
    "n_small = 4\n",
    "\n",
    "for i in range(n_small):\n",
    "    for j in range(n_timesteps // n_small):\n",
    "        model_loss.get_layer(f'z1_{n_timesteps // n_small * i}').set_weights(m_small.get_layer(f'z1_{i}').get_weights())\n",
    "        model_loss.get_layer(f'z2_{n_timesteps // n_small * i}').set_weights(m_small.get_layer(f'z2_{i}').get_weights())\n",
    "        model_loss.get_layer(f'z1_{n_timesteps // n_small * i + j}').set_weights(m_small.get_layer(f'z1_{i}').get_weights())\n",
    "        model_loss.get_layer(f'z2_{n_timesteps // n_small * i + j}').set_weights(m_small.get_layer(f'z2_{i}').get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc050503",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "41902956",
   "metadata": {},
   "outputs": [],
   "source": [
    "dW = tf.sqrt(dt) * tf.random.normal((n_paths, n_timesteps, n_diffusion_factors))\n",
    "dN = tf.random.poisson((n_paths, n_timesteps), tf.constant(dt * np.array([1., 1.]).transpose().reshape(-1)))\n",
    "target = tf.zeros((n_paths, n_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9e06d098",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check for exploding gradients before training\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    loss = mse(model_loss([dW, dN]), target)\n",
    "\n",
    "# bias of the last dense layer\n",
    "variables = model_loss.variables[-1]\n",
    "tape.gradient(loss, variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "e82e39a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensorboard log dir\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "model_name = f\"{timestamp}__{n_dimensions}__{n_paths}__{n_timesteps}__{T}\"\n",
    "log_dir = \"/home/tmp/starokon/tensorboard/\" + model_name\n",
    "\n",
    "# callbacks\n",
    "# checkpoint_callback = ModelCheckpoint('_models/weights{epoch:04d}.h5', save_weights_only=True, overwrite=True)\n",
    "checkpoint_callback = ModelCheckpoint(f'_output/{model_name}/model.h5', save_weights_only=True, save_best_only=True, overwrite=True)\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1, profile_batch=1)\n",
    "nan_callback = tf.keras.callbacks.TerminateOnNaN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6692cec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(edgeitems=11, linewidth=90, formatter=dict(float=lambda x: \"%7.5g\" % x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c12ff294",
   "metadata": {},
   "outputs": [],
   "source": [
    "adam = Adam(learning_rate=1e-3)\n",
    "model_loss.compile(loss='mse', optimizer=adam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "c3ba4c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Y0Callback(Callback):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Y0Callback, self).__init__()\n",
    "        self.y0s = []\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        y0 = model_loss.variables[1].numpy()[0]\n",
    "        self.y0s += [y0]\n",
    "        print(y0 + \"\\n\")\n",
    "        \n",
    "    def on_train_end(self, logs=None):\n",
    "        pass\n",
    "\n",
    "y0_callback = Y0Callback()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbf6f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model_loss.fit([dW, dN], target, batch_size=128, initial_epoch=0, epochs=100, callbacks=[nan_callback, checkpoint_callback, tensorboard_callback, y0_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0e3f366c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256/256 [==============================] - 4s 2ms/step - loss: 3.4178e-09\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3.417778726699794e-09"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# validate\n",
    "\n",
    "dW_test = tf.sqrt(dt) * tf.random.normal((n_paths//8, n_timesteps, n_diffusion_factors))\n",
    "dN_test = tf.random.poisson((n_paths//8, n_timesteps), tf.constant(dt * np.array([1., 1.]).transpose().reshape(-1)))\n",
    "target_test = tf.zeros((n_paths//8, n_dimensions))\n",
    "\n",
    "model_loss.evaluate([dW_test, dN_test], target_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1ba9886",
   "metadata": {},
   "source": [
    "# Display paths and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a1c161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load bad model\n",
    "model_loss.load_weights('_models/weights0011.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e765bc4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-4.21302719e-03, -1.83777374e-04,  7.68974721e-02, ...,\n",
       "        -1.08535380e-04, -3.65010686e-02, -1.43885612e-04],\n",
       "       [ 3.12728644e-03, -1.86214020e-04,  1.65139437e-02, ...,\n",
       "        -1.07569365e-04,  9.47822630e-02, -1.43885612e-04],\n",
       "       [-1.13846944e-03, -1.84155477e-04,  3.47495675e-02, ...,\n",
       "        -1.07487824e-04, -5.74785471e-03, -1.43885612e-04],\n",
       "       ...,\n",
       "       [-4.19508014e-03, -1.82968608e-04,  9.62089002e-03, ...,\n",
       "        -1.07971777e-04, -5.63402846e-03, -1.43885612e-04],\n",
       "       [-1.20397564e-02, -1.82203512e-04,  1.12469167e-01, ...,\n",
       "        -1.08954977e-04, -2.62902267e-02, -1.43885612e-04],\n",
       "       [-3.71439802e-03, -1.83611395e-04,  4.11325842e-02, ...,\n",
       "        -1.07773740e-04,  1.11734495e-02, -1.43885612e-04]], dtype=float32)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = model_loss([dW, dN]).numpy()\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "28f074bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = model_paths([dW, dN]).numpy()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
